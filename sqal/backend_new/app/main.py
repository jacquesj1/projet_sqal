"""
SQAL Backend - FastAPI Application
Production-ready backend for foie gras quality control system
"""
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Response
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from sqlalchemy import select, func
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Set
from dateutil import parser
from pydantic import ValidationError

# Import database components
from app.core.database import (
    init_db,
    create_hypertable,
    create_continuous_aggregate,
    add_compression_policy,
    add_retention_policy,
    AsyncSessionLocal,
    close_db
)
from app.models.sensor import SensorSample, DeviceStatus, QualityAlert

# Import validation schemas
from app.schemas.sensor_data import SensorDataMessage

# Import rate limiter
from app.core.rate_limiter import RateLimiter

# Import cache manager
from app.core.cache import CacheManager
import os

# Import metrics
from app.core.metrics import (
    initialize_metrics,
    prometheus_middleware,
    get_metrics,
    record_sample_analyzed,
    update_websocket_connections,
    record_websocket_message,
    system_uptime_seconds
)

# Import health checks
from app.core.health import (
    health_manager,
    initialize_health_checks
)

# Import graceful shutdown
from app.core.graceful_shutdown import (
    shutdown_handler,
    initialize_graceful_shutdown,
    GracefulShutdownMiddleware,
    cleanup_websockets
)

# Import circuit breaker
from app.core.circuit_breaker import (
    db_circuit_breaker,
    cache_circuit_breaker,
    CircuitBreakerOpenError
)

# Import blockchain
from app.core.blockchain import certify_quality_analysis

# Import routers
from app.routers import ai, firmware, bug_tracking, admin, reports, analysis, sensors, organizations, blockchain

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Store for connected WebSocket clients with subscriptions
from typing import Dict, List
from dataclasses import dataclass

@dataclass
class DashboardSubscription:
    """Subscription filters for selective broadcasting"""
    websocket: WebSocket
    device_ids: List[str] = None  # None = all devices
    grades: List[str] = None  # None = all grades (A+, A, B, C, REJECT)
    min_score: float = 0.0  # Minimum score to receive

connected_dashboards: Dict[WebSocket, DashboardSubscription] = {}
latest_sample = {}
all_samples = []

# Initialize rate limiter (100 messages per minute per client)
rate_limiter = RateLimiter(max_requests=100, window_seconds=60)

# Initialize cache manager (will be connected in lifespan)
cache_manager: CacheManager = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager for startup/shutdown"""
    logger.info("üöÄ SQAL Backend starting...")

    # Initialize Prometheus metrics
    initialize_metrics()

    # Initialize health checks
    initialize_health_checks()

    # Initialize graceful shutdown
    initialize_graceful_shutdown()

    # Register WebSocket cleanup
    shutdown_handler.register_cleanup(
        "websockets",
        lambda: cleanup_websockets(connected_dashboards)
    )

    # Track startup time
    import time as time_module
    startup_time = time_module.time()

    # Wait for TimescaleDB to be ready
    import asyncio
    max_retries = 10
    retry_delay = 2  # seconds
    
    for attempt in range(max_retries):
        try:
            logger.info(f"‚è≥ Attempting to connect to TimescaleDB (attempt {attempt + 1}/{max_retries})...")
            
            # Initialize database
            await init_db()
            
            # Create hypertables
            await create_hypertable("sensor_samples", "timestamp")
            await create_hypertable("device_status", "timestamp")
            await create_hypertable("quality_alerts", "timestamp")
            
            # Create continuous aggregates for real-time analytics
            await create_continuous_aggregate(
                view_name="sensor_data_hourly",
                source_table="sensor_samples",
                time_column="timestamp",
                bucket_interval="1 hour",
                select_clause="""
                    device_id,
                    COUNT(*) as sample_count,
                    AVG(fusion_final_score) as avg_quality,
                    MIN(fusion_final_score) as min_quality,
                    MAX(fusion_final_score) as max_quality,
                    COUNT(*) FILTER (WHERE fusion_final_grade = 'A+') as grade_a_plus,
                    COUNT(*) FILTER (WHERE fusion_final_grade = 'A') as grade_a,
                    COUNT(*) FILTER (WHERE fusion_final_grade = 'B') as grade_b,
                    COUNT(*) FILTER (WHERE fusion_final_grade = 'C') as grade_c,
                    COUNT(*) FILTER (WHERE fusion_final_grade = 'REJECT') as grade_reject
                """
            )
            
            await create_continuous_aggregate(
                view_name="sensor_data_daily",
                source_table="sensor_samples",
                time_column="timestamp",
                bucket_interval="1 day",
                select_clause="""
                    device_id,
                    COUNT(*) as sample_count,
                    AVG(fusion_final_score) as avg_quality,
                    COUNT(*) FILTER (WHERE fusion_final_grade IN ('C', 'REJECT')) as defect_count
                """
            )
            
            # Add compression policy (compress data older than 7 days)
            await add_compression_policy("sensor_samples", compress_after="7 days")
            
            # Add retention policy (keep data for 90 days)
            await add_retention_policy("sensor_samples", retain_for="90 days")
            
            logger.info("‚úÖ TimescaleDB initialized with hypertables and policies")
            break  # Success, exit retry loop

        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"‚ö†Ô∏è Failed to connect to TimescaleDB: {e}")
                logger.info(f"‚è≥ Retrying in {retry_delay} seconds...")
                await asyncio.sleep(retry_delay)
            else:
                logger.error(f"‚ùå Failed to initialize database after {max_retries} attempts: {e}", exc_info=True)
                logger.warning("‚ö†Ô∏è Backend will start without database (data will be stored in memory only)")

    # Initialize Redis cache
    global cache_manager
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    cache_manager = CacheManager(redis_url)

    try:
        await cache_manager.connect()
        logger.info("‚úÖ Redis cache initialized")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Redis cache initialization failed: {e}")
        logger.warning("‚ö†Ô∏è Backend will run without cache (degraded performance)")

    # Start uptime tracking
    async def update_uptime():
        while True:
            uptime = time_module.time() - startup_time
            system_uptime_seconds.set(uptime)
            await asyncio.sleep(60)  # Update every minute

    # Start uptime tracking task
    uptime_task = asyncio.create_task(update_uptime())

    # Mark application as started and ready
    health_manager.mark_as_started()
    health_manager.mark_as_ready()

    logger.info("‚úÖ SQAL Backend fully started and ready!")

    yield

    # ========================================================================
    # SHUTDOWN PHASE
    # ========================================================================

    logger.info("üõë SQAL Backend shutting down...")

    # Mark as shutting down
    health_manager.mark_as_shutting_down()

    # Execute graceful shutdown
    await shutdown_handler.shutdown()

    # Cancel uptime tracking
    uptime_task.cancel()
    try:
        await uptime_task
    except asyncio.CancelledError:
        pass

    # Disconnect cache
    if cache_manager:
        await cache_manager.disconnect()

    await close_db()


# Create FastAPI app
app = FastAPI(
    title="SQAL Backend API",
    description="Real-time quality control system for foie gras production",
    version="2.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000", "*"],  # Allow Docker network connections
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Prometheus metrics middleware
from starlette.middleware.base import BaseHTTPMiddleware
app.add_middleware(BaseHTTPMiddleware, dispatch=prometheus_middleware)

# Graceful shutdown middleware (tracks active requests)
app.add_middleware(GracefulShutdownMiddleware)

# Include routers
app.include_router(ai.router)
app.include_router(firmware.router)
app.include_router(bug_tracking.router)
app.include_router(admin.router)
app.include_router(reports.router)
app.include_router(analysis.router)
app.include_router(sensors.router)
app.include_router(organizations.router)
app.include_router(blockchain.router)


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "SQAL Backend",
        "version": "2.0.0",
        "timestamp": datetime.utcnow().isoformat()
    }


@app.get("/health")
async def health_check():
    """Detailed health check"""
    try:
        # Check database connection
        async with AsyncSessionLocal() as db:
            result = await db.execute(select(func.count()).select_from(SensorSample))
            total_samples = result.scalar()
            db_status = "ok"
    except Exception as e:
        logger.error(f"Database health check failed: {e}")
        total_samples = 0
        db_status = "error"

    return {
        "status": "healthy",
        "components": {
            "api": "ok",
            "websocket": "ok",
            "database": db_status,
            "timescaledb": db_status
        },
        "connected_dashboards": len(connected_dashboards),
        "samples_in_memory": len(all_samples),
        "samples_in_db": total_samples
    }


@app.get("/metrics")
async def metrics_endpoint():
    """
    Prometheus metrics endpoint

    Returns metrics in Prometheus text format for scraping by Prometheus server.

    Available metrics:
    - http_requests_total: Total HTTP requests by method, endpoint, status
    - http_request_duration_seconds: Request latency histogram
    - cache_hits_total / cache_misses_total: Cache performance
    - samples_analyzed_total: Total samples analyzed by device and grade
    - sample_quality_score: Quality score distribution
    - db_connections_active: Active database connections
    - websocket_connections_active: Active WebSocket connections
    - conformity_rate_percent: Production conformity rate
    - production_throughput_samples_per_hour: Production throughput

    Usage:
        curl http://localhost:8000/metrics
    """
    from fastapi import Response
    metrics_data, content_type = get_metrics()
    return Response(content=metrics_data, media_type=content_type)


# ============================================================================
# Advanced Health Check Endpoints (Kubernetes-ready)
# ============================================================================

@app.get("/health/live")
async def health_liveness():
    """
    Liveness probe - Is the application alive?

    Kubernetes uses this to determine if the pod should be restarted.
    Should return quickly (< 1s) and check basic functionality.

    Returns:
        200: Application is alive
        503: Application is dead (Kubernetes will restart)
    """
    result = await health_manager.check_liveness()

    status_code = 200 if result["status"] == "healthy" else 503

    return Response(
        content=json.dumps(result),
        media_type="application/json",
        status_code=status_code
    )


@app.get("/health/ready")
async def health_readiness():
    """
    Readiness probe - Can the application serve traffic?

    Kubernetes uses this to determine if pod should receive traffic.
    Checks all dependencies (database, cache, etc.).

    Returns:
        200: Application is ready
        503: Application is not ready (removed from load balancer)
    """
    result = await health_manager.check_readiness()

    status_code = 200 if result["status"] in ["ready", "degraded"] else 503

    return Response(
        content=json.dumps(result),
        media_type="application/json",
        status_code=status_code
    )


@app.get("/health/startup")
async def health_startup():
    """
    Startup probe - Has the application finished starting?

    Kubernetes uses this to know when to start liveness/readiness probes.
    Returns immediately after startup is complete.

    Returns:
        200: Application has started
        503: Application is still starting
    """
    result = await health_manager.check_startup()

    status_code = 200 if result["status"] == "started" else 503

    return Response(
        content=json.dumps(result),
        media_type="application/json",
        status_code=status_code
    )


@app.get("/health/detailed")
async def health_detailed():
    """
    Detailed health check with all information

    Includes:
    - Overall status
    - Startup status
    - Liveness status
    - Readiness status
    - All component checks
    - Uptime

    Usage:
        curl http://localhost:8000/health/detailed
    """
    result = await health_manager.check_health_detailed()
    return result


@app.get("/health/circuit-breakers")
async def circuit_breakers_status():
    """
    Get circuit breaker states

    Shows state of all circuit breakers:
    - closed: Normal operation
    - open: Too many failures, failing fast
    - half_open: Testing recovery

    Usage:
        curl http://localhost:8000/health/circuit-breakers
    """
    from fastapi import Response
    return {
        "database": db_circuit_breaker.get_state(),
        "cache": cache_circuit_breaker.get_state(),
        "timestamp": datetime.utcnow().isoformat()
    }


# ============================================================================
# WebSocket Endpoints
# ============================================================================

@app.websocket("/ws/sensors/")
async def websocket_sensors(websocket: WebSocket):
    """
    WebSocket endpoint for data_generator
    Receives sensor data from ESP32 simulators
    """
    await websocket.accept()
    client_id = f"{websocket.client.host}:{websocket.client.port}"
    logger.info(f"üì° Data generator connected: {client_id}")

    # Track WebSocket connection
    update_websocket_connections("/ws/sensors/", +1)

    # Create database session
    db = AsyncSessionLocal()

    # Send confirmation
    await websocket.send_json({
        "type": "connection_established",
        "message": "FastAPI backend ready to receive sensor data"
    })

    try:
        while True:
            # Receive data from generator
            logger.info(f"‚è≥ Waiting for message from {client_id}...")
            data = await websocket.receive_json()
            logger.info(f"üì® Received message from {client_id}: type={data.get('type')}, sample_id={data.get('sample_id', 'unknown')}, mode={data.get('mode', 'unknown')}")

            # Track inbound message
            record_websocket_message("/ws/sensors/", "inbound")

            msg_type = data.get("type")

            if msg_type == "sensor_data":
                logger.info(f"‚úÖ Processing sensor_data message for sample {data.get('sample_id', 'unknown')}")
                # ‚úÖ RATE LIMITING - Check before processing
                if not rate_limiter.is_allowed(client_id):
                    logger.warning(f"‚ö†Ô∏è Rate limit exceeded for {client_id}")
                    usage = rate_limiter.get_usage(client_id)

                    await websocket.send_json({
                        "type": "rate_limit_exceeded",
                        "message": f"Rate limit exceeded. Max {rate_limiter.max_requests} requests per {rate_limiter.window_seconds}s",
                        "usage": usage,
                        "retry_after_seconds": rate_limiter.window_seconds
                    })

                    # Slow down the generator
                    import asyncio
                    await asyncio.sleep(1)
                    continue
                
                # Determine if data is raw or analyzed FIRST (before validation)
                data_mode = data.get("mode", "analyzed")  # Default: analyzed (backward compatible)
                logger.debug(f"VL53L8CH keys: {list(data.get('vl53l8ch', {}).keys())}")
                logger.debug(f"AS7341 keys: {list(data.get('as7341', {}).keys())}")
                # ‚úÖ VALIDATE DATA WITH PYDANTIC (only for analyzed mode)
                if data_mode == "analyzed":
                    try:
                        validated_data = SensorDataMessage(**data)
                        logger.debug(f"‚úÖ Data validation passed for sample {validated_data.sample_id}")
                    except ValidationError as e:
                        logger.error(f"‚ùå DATA VALIDATION FAILED")
                        logger.error(f"Sample ID: {data.get('sample_id', 'unknown')}")
                        logger.error(f"Validation errors ({len(e.errors())} errors):")
                        for error in e.errors():
                            logger.error(f"  - {error['loc']}: {error['msg']}")

                        # Send error response to generator
                        await websocket.send_json({
                            "type": "validation_error",
                            "sample_id": data.get('sample_id', 'unknown'),
                            "message": "Data validation failed",
                            "errors": e.errors()
                        })
                        continue  # Skip this sample, wait for next one
                else:
                    # Raw mode: skip Pydantic validation, will be validated after analysis
                    logger.debug(f"üì• Raw data received for sample {data.get('sample_id', 'unknown')}")

                # LOG: Data received from data_generator
                logger.info("=" * 80)
                logger.info("üì• DATA RECEIVED FROM DATA_GENERATOR")
                logger.info(f"Device ID: {data.get('device_id', 'unknown')}")
                logger.info(f"Sample ID: {data.get('sample_id', 'unknown')}")
                logger.info(f"Mode: {data_mode}")
                logger.info(f"Timestamp: {data.get('timestamp', 'unknown')}")
                
                # Log VL53L8CH data summary
                vl53l8ch_data = data.get("VL53L8CH", {})
                # Handle both uppercase and lowercase keys (backward compatibility)
                if not vl53l8ch_data:
                    vl53l8ch_data = data.get("vl53l8ch", {})
                
                logger.info(f"\nüî∑ VL53L8CH (ToF) Analysis:")
                logger.info(f"  Grade: {vl53l8ch_data.get('grade', 'N/A')}")
                logger.info(f"  Quality Score: {vl53l8ch_data.get('quality_score', 0):.3f}")
                logger.info(f"  Volume: {vl53l8ch_data.get('volume_mm3', 0):.1f} mm¬≥")
                logger.info(f"  Avg Height: {vl53l8ch_data.get('avg_height_mm', 0):.1f} mm")
                logger.info(f"  Surface Uniformity: {vl53l8ch_data.get('surface_uniformity', 0):.3f}")
                logger.info(f"  Defects: {len(vl53l8ch_data.get('defects', []))}")
                
                # Log advanced VL53L8CH metrics if present
                if 'bins_analysis' in vl53l8ch_data:
                    logger.info(f"  üìä Bins Analysis: Present")
                if 'reflectance_analysis' in vl53l8ch_data:
                    logger.info(f"  üìä Reflectance Analysis: Present")
                if 'amplitude_consistency' in vl53l8ch_data:
                    logger.info(f"  üìä Amplitude Consistency: Present")
                
                # Log AS7341 data summary
                as7341_data = data.get("AS7341", {})
                # Handle both uppercase and lowercase keys (backward compatibility)
                if not as7341_data:
                    as7341_data = data.get("as7341", {})
                
                logger.info(f"\nüî∂ AS7341 (Spectral) Analysis:")
                logger.info(f"  Grade: {as7341_data.get('grade', 'N/A')}")
                logger.info(f"  Quality Score: {as7341_data.get('quality_score', 0):.3f}")
                logger.info(f"  Freshness Index: {as7341_data.get('freshness_index', 0):.3f}")
                logger.info(f"  Fat Quality Index: {as7341_data.get('fat_quality_index', 0):.3f}")
                logger.info(f"  Oxidation Index: {as7341_data.get('oxidation_index', 0):.3f}")
                logger.info(f"  Color Uniformity: {as7341_data.get('color_uniformity', 0):.3f}")
                logger.info(f"  Defects: {len(as7341_data.get('defects', []))}")
                
                # Log advanced AS7341 metrics if present
                if 'spectral_analysis' in as7341_data:
                    logger.info(f"  üìä Spectral Analysis: Present")
                if 'color_analysis' in as7341_data:
                    logger.info(f"  üìä Color Analysis (L*a*b*): Present")
                
                # Log Fusion data summary
                fusion_data = data.get("fusion", {})
                logger.info(f"\nüî∑üî∂ FUSION Result:")
                logger.info(f"  Final Grade: {fusion_data.get('final_grade', 'N/A')}")
                logger.info(f"  Final Score: {fusion_data.get('final_score', 0):.3f}")
                logger.info(f"  VL53L8CH Contribution: {fusion_data.get('vl53l8ch_score', 0):.3f}")
                logger.info(f"  AS7341 Contribution: {fusion_data.get('as7341_score', 0):.3f}")
                logger.info(f"  Total Defects: {len(fusion_data.get('defects', []))}")
                
                # Log metadata if present
                meta = data.get("meta", {})
                if meta:
                    logger.info(f"\nüìã Metadata:")
                    logger.info(f"  Firmware: {meta.get('firmware_version', 'N/A')}")
                    logger.info(f"  Temperature: {meta.get('temperature_c', 0):.1f}¬∞C")
                    logger.info(f"  Humidity: {meta.get('humidity_percent', 0):.1f}%")
                    logger.info(f"  Config Profile: {meta.get('config_profile', 'N/A')}")
                
                # Log complete JSON structure keys
                logger.info(f"\nüì¶ Complete JSON Structure:")
                logger.info(f"  Top-level keys: {list(data.keys())}")
                if vl53l8ch_data:
                    logger.info(f"  VL53L8CH keys: {list(vl53l8ch_data.keys())[:10]}...")  # First 10
                if as7341_data:
                    logger.info(f"  AS7341 keys: {list(as7341_data.keys())[:10]}...")  # First 10
                
                logger.info("=" * 80)
                
                if data_mode == "raw":
                    # RAW DATA MODE: For now, use data as-is since simulator already provides fusion preview
                    # TODO: Implement full raw data analysis pipeline when needed
                    logger.info(f"üî¨ Raw data mode - using fusion preview from {data.get('device_id', 'unknown')}")
                    analyzed_data = data
                else:
                    # ANALYZED DATA MODE: Use as-is (backward compatible)
                    analyzed_data = data
                
                logger.info(
                    f"‚úÖ Donn√©es analys√©es | "
                    f"Device: {analyzed_data.get('device_id', 'unknown')} | "
                    f"Grade: {analyzed_data.get('fusion', {}).get('final_grade', 'N/A')} | "
                    f"Score: {analyzed_data.get('fusion', {}).get('final_score', 0):.3f}"
                )
                
                # Store data in memory
                global latest_sample
                latest_sample = analyzed_data
                all_samples.append(analyzed_data)
                
                # Save to TimescaleDB with blockchain certification
                try:
                    logger.info(f"üíæ Attempting to save sample {analyzed_data.get('sample_id')} to database...")
                    blockchain_data = await save_sensor_sample(db, analyzed_data)
                    logger.info(f"‚úÖ Successfully saved sample {analyzed_data.get('sample_id')} to database")

                    # Add blockchain data to analyzed_data for frontend
                    if blockchain_data:
                        analyzed_data["blockchain"] = blockchain_data
                        logger.debug(f"‚úÖ Blockchain data added to response")
                except Exception as e:
                    logger.error(f"‚ùå Failed to save to database: {e}", exc_info=True)
                    logger.error(f"‚ùå Sample data: device_id={analyzed_data.get('device_id')}, sample_id={analyzed_data.get('sample_id')}")

                # Broadcast to all connected dashboards (with blockchain data)
                await broadcast_to_dashboards(analyzed_data)
                
            elif msg_type == "ping":
                await websocket.send_json({"type": "pong"})
                
    except WebSocketDisconnect:
        logger.info(f"üì° Data generator disconnected: {websocket.client}")
    except Exception as e:
        logger.error(f"‚ùå Error in sensor WebSocket: {e}", exc_info=True)
    finally:
        # Track WebSocket disconnection
        update_websocket_connections("/ws/sensors/", -1)
        await db.close()


async def save_sensor_sample(db: AsyncSessionLocal, data: dict) -> dict:
    """
    Save sensor sample to TimescaleDB with complete analysis data

    Returns:
        dict: Updated data with blockchain certification (hash, QR code, timestamp)
    """
    try:
        # Start timer for metrics
        start_time = time.time()

        # Parse timestamp
        timestamp = parser.parse(data.get("timestamp"))

        # Extract data (handle both uppercase and lowercase keys)
        vl53l8ch = data.get("VL53L8CH") or data.get("vl53l8ch", {})
        as7341 = data.get("AS7341") or data.get("as7341", {})
        fusion = data.get("fusion", {})
        meta = data.get("meta", {})

        # Create SensorSample record with ALL fields
        sample = SensorSample(
            timestamp=timestamp,
            device_id=data.get("device_id"),
            sample_id=data.get("sample_id"),

            # VL53L8CH raw data
            vl53l8ch_distance_matrix=vl53l8ch.get("distance_matrix"),
            vl53l8ch_reflectance_matrix=vl53l8ch.get("reflectance_matrix"),
            vl53l8ch_amplitude_matrix=vl53l8ch.get("amplitude_matrix"),

            # VL53L8CH basic metrics
            vl53l8ch_volume_mm3=vl53l8ch.get("volume_mm3"),
            vl53l8ch_avg_height_mm=vl53l8ch.get("average_height_mm") or vl53l8ch.get("avg_height_mm"),
            vl53l8ch_max_height_mm=vl53l8ch.get("max_height_mm"),
            vl53l8ch_min_height_mm=vl53l8ch.get("min_height_mm"),
            vl53l8ch_base_area_mm2=vl53l8ch.get("base_area_mm2"),
            vl53l8ch_surface_uniformity=vl53l8ch.get("surface_uniformity"),
            vl53l8ch_defect_count=len(vl53l8ch.get("defects", [])),
            vl53l8ch_quality_score=vl53l8ch.get("quality_score"),
            vl53l8ch_grade=vl53l8ch.get("grade"),

            # VL53L8CH detailed analysis
            vl53l8ch_bins_analysis=vl53l8ch.get("bins_analysis"),
            vl53l8ch_reflectance_analysis=vl53l8ch.get("reflectance_analysis"),
            vl53l8ch_amplitude_consistency=vl53l8ch.get("amplitude_consistency"),
            vl53l8ch_score_breakdown=vl53l8ch.get("score_breakdown"),
            vl53l8ch_defects=vl53l8ch.get("defects"),

            # AS7341 raw data
            as7341_channels=as7341.get("channels"),
            as7341_integration_time=as7341.get("integration_time"),
            as7341_gain=as7341.get("gain"),

            # AS7341 basic metrics
            as7341_color_score=as7341.get("color_score"),
            as7341_freshness_score=as7341.get("freshness_score"),
            as7341_freshness_index=as7341.get("freshness_index"),
            as7341_fat_quality_index=as7341.get("fat_quality_index"),
            as7341_oxidation_index=as7341.get("oxidation_index"),
            as7341_color_uniformity=as7341.get("color_uniformity"),
            as7341_quality_score=as7341.get("quality_score"),
            as7341_grade=as7341.get("grade"),

            # AS7341 detailed analysis
            as7341_spectral_analysis=as7341.get("spectral_analysis"),
            as7341_color_analysis=as7341.get("color_analysis"),
            as7341_score_breakdown=as7341.get("score_breakdown"),
            as7341_defects=as7341.get("defects"),

            # Fusion results
            fusion_final_score=fusion.get("final_score", 0.0),
            fusion_final_grade=fusion.get("final_grade", "UNKNOWN"),
            fusion_vl53l8ch_score=fusion.get("vl53l8ch_score"),
            fusion_as7341_score=fusion.get("as7341_score"),
            fusion_confidence=fusion.get("confidence"),
            fusion_defects=fusion.get("defects"),

            # Metadata
            meta_firmware_version=meta.get("firmware_version"),
            meta_temperature_c=meta.get("temperature_c"),
            meta_humidity_percent=meta.get("humidity_percent"),
            meta_config_profile=meta.get("config_profile"),
            processing_time_ms=data.get("processing_time_ms", 0.0)
        )

        # ============================================================================
        # BLOCKCHAIN CERTIFICATION - Generate hash and QR code for traceability
        # ============================================================================
        try:
            blockchain_cert = certify_quality_analysis(
                sample_id=data.get("sample_id"),
                device_id=data.get("device_id"),
                vl53l8ch_score=vl53l8ch.get("quality_score", 0.0),
                as7341_score=as7341.get("quality_score", 0.0),
                fusion_final_score=fusion.get("final_score", 0.0),
                fusion_final_grade=fusion.get("final_grade", "UNKNOWN"),
                defects=fusion.get("defects", []),
                lot_abattage=data.get("lot_abattage"),
                eleveur=data.get("eleveur"),
                provenance=data.get("provenance"),
                generate_qr=True
            )

            # Add blockchain data to sample
            sample.blockchain_hash = blockchain_cert["blockchain_hash"]
            sample.blockchain_timestamp = parser.parse(blockchain_cert["blockchain_timestamp"])
            sample.qr_code_base64 = blockchain_cert.get("qr_code_base64")
            sample.lot_abattage = data.get("lot_abattage")
            sample.eleveur = data.get("eleveur")
            sample.provenance = data.get("provenance")

            logger.info(f"üîê Blockchain certified: {blockchain_cert['blockchain_hash'][:16]}...")
        except Exception as blockchain_error:
            logger.warning(f"‚ö†Ô∏è Blockchain certification failed (non-blocking): {blockchain_error}")
            # Continue without blockchain - it's not critical for the analysis

        db.add(sample)
        await db.commit()

        # Calculate processing time
        processing_time = time.time() - start_time

        # Record metrics
        record_sample_analyzed(
            device_id=data.get("device_id"),
            grade=fusion.get("final_grade", "UNKNOWN"),
            quality_score=fusion.get("final_score", 0.0),
            processing_time=processing_time
        )

        logger.debug(f"üíæ Sample saved to database with complete analysis: {data.get('sample_id')}")

        # Invalidate cached metrics after new data arrives
        if cache_manager:
            # TODO: Implement these cache invalidation methods in CacheManager
            # await cache_manager.invalidate_dashboard_metrics()
            # await cache_manager.invalidate_foie_gras_metrics()
            # await cache_manager.set_latest_sample(data, ttl=10)
            logger.debug("‚ôªÔ∏è Cache invalidation skipped (methods not implemented)")

        # Return blockchain certification data for frontend
        return {
            "blockchain_hash": sample.blockchain_hash,
            "blockchain_timestamp": sample.blockchain_timestamp.isoformat() if sample.blockchain_timestamp else None,
            "qr_code_base64": sample.qr_code_base64,
            "lot_abattage": sample.lot_abattage,
            "eleveur": sample.eleveur,
            "provenance": sample.provenance
        }

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to save sample: {e}", exc_info=True)
        raise


@app.websocket("/ws/realtime/")
async def websocket_dashboard(websocket: WebSocket):
    """
    WebSocket endpoint for frontend dashboard with selective broadcasting

    Clients can subscribe to specific filters:
    - device_ids: List of device IDs to receive updates for
    - grades: List of grades to receive (A+, A, B, C, REJECT)
    - min_score: Minimum quality score to receive

    Example subscription:
    {
        "command": "subscribe",
        "device_ids": ["device-001"],
        "grades": ["C", "REJECT"],
        "min_score": 0.0
    }
    """
    await websocket.accept()

    # Track WebSocket connection
    update_websocket_connections("/ws/realtime/", +1)

    # Create default subscription (receive everything)
    subscription = DashboardSubscription(
        websocket=websocket,
        device_ids=None,
        grades=None,
        min_score=0.0
    )
    connected_dashboards[websocket] = subscription

    logger.info(f"üìä Dashboard connected: {websocket.client} | Total: {len(connected_dashboards)}")

    # Send connection confirmation
    await websocket.send_json({
        "type": "connection_established",
        "message": "Connected to real-time sensor feed",
        "timestamp": datetime.utcnow().isoformat()
    })

    # Send latest sample if available
    if latest_sample:
        await websocket.send_json({
            "type": "sensor_update",
            "fusion": latest_sample.get("fusion"),
            "vl53l8ch": latest_sample.get("vl53l8ch"),
            "as7341": latest_sample.get("as7341"),
            "meta": latest_sample.get("meta")
        })

    try:
        while True:
            # Keep connection alive and handle incoming messages
            data = await websocket.receive_json()

            # Track inbound message
            record_websocket_message("/ws/realtime/", "inbound")

            command = data.get("command")

            if command == "subscribe":
                # Update subscription filters
                subscription.device_ids = data.get("device_ids")
                subscription.grades = data.get("grades")
                subscription.min_score = data.get("min_score", 0.0)

                logger.info(f"üìä Dashboard {websocket.client} updated subscription: "
                          f"devices={subscription.device_ids}, "
                          f"grades={subscription.grades}, "
                          f"min_score={subscription.min_score}")

                await websocket.send_json({
                    "type": "subscription_updated",
                    "filters": {
                        "device_ids": subscription.device_ids,
                        "grades": subscription.grades,
                        "min_score": subscription.min_score
                    }
                })

            elif command == "get_latest":
                if latest_sample and _matches_subscription(latest_sample, subscription):
                    await websocket.send_json({
                        "type": "sensor_data",
                        "payload": latest_sample
                    })

    except WebSocketDisconnect:
        connected_dashboards.pop(websocket, None)
        update_websocket_connections("/ws/realtime/", -1)
        logger.info(f"üìä Dashboard disconnected: {websocket.client} | Remaining: {len(connected_dashboards)}")
    except Exception as e:
        connected_dashboards.pop(websocket, None)
        update_websocket_connections("/ws/realtime/", -1)
        logger.error(f"‚ùå Error in dashboard WebSocket: {e}", exc_info=True)


def _matches_subscription(data: dict, subscription: DashboardSubscription) -> bool:
    """
    Check if data matches subscription filters

    Returns True if data should be sent to this subscriber
    """
    # Extract data
    device_id = data.get("device_id")
    fusion_data = data.get("fusion", {})
    grade = fusion_data.get("final_grade")
    score = fusion_data.get("final_score", 0.0)

    # Filter by device_ids
    if subscription.device_ids and device_id not in subscription.device_ids:
        return False

    # Filter by grades
    if subscription.grades and grade not in subscription.grades:
        return False

    # Filter by min_score
    if score < subscription.min_score:
        return False

    return True


async def broadcast_to_dashboards(data: dict):
    """
    Broadcast sensor data to connected dashboard clients with selective filtering

    Only sends data to clients whose subscription filters match
    """
    if not connected_dashboards:
        return

    # LOG: Data being sent to frontend
    logger.info("=" * 80)
    logger.info("üì§ DATA SENT TO FRONTEND DASHBOARDS (SELECTIVE BROADCAST)")
    logger.info(f"Connected dashboards: {len(connected_dashboards)}")
    logger.info(f"Device ID: {data.get('device_id', 'unknown')}")
    logger.info(f"Sample ID: {data.get('sample_id', 'unknown')}")

    # Log what's being sent (handle both uppercase and lowercase keys)
    fusion_data = data.get("fusion", {})
    vl53l8ch_data = data.get("VL53L8CH") or data.get("vl53l8ch", {})
    as7341_data = data.get("AS7341") or data.get("as7341", {})

    logger.info(f"\nüéØ Summary for Frontend:")
    logger.info(f"  Final Grade: {fusion_data.get('final_grade', 'N/A')}")
    logger.info(f"  Final Score: {fusion_data.get('final_score', 0):.3f}")
    logger.info(f"  VL53L8CH Grade: {vl53l8ch_data.get('grade', 'N/A')}")
    logger.info(f"  AS7341 Grade: {as7341_data.get('grade', 'N/A')}")
    logger.info(f"  Total Defects: {fusion_data.get('total_defect_count', len(fusion_data.get('defects', [])))}")

    logger.info(f"\nüìä Metrics Being Sent:")
    logger.info(f"  Volume: {vl53l8ch_data.get('volume_mm3', 0):.1f} mm¬≥")
    logger.info(f"  Avg Height: {vl53l8ch_data.get('avg_height_mm', 0):.1f} mm")
    logger.info(f"  Freshness: {as7341_data.get('freshness_index', 0):.3f}")
    logger.info(f"  Fat Quality: {as7341_data.get('fat_quality_index', 0):.3f}")

    # ‚úÖ Frontend attend "sensor_update" avec donn√©es au niveau racine
    message = {
        "type": "sensor_update",
        "timestamp": data.get("timestamp"),
        "device_id": data.get("device_id"),
        "sample_id": data.get("sample_id"),
        "vl53l8ch": vl53l8ch_data,
        "as7341": as7341_data,
        "fusion": fusion_data,
        "blockchain": data.get("blockchain"),
        "meta": data.get("meta", {})
    }

    # Send to matching dashboards only (selective broadcast)
    sent_count = 0
    filtered_count = 0
    disconnected = []

    for ws, subscription in connected_dashboards.items():
        # Check if data matches subscription filters
        if not _matches_subscription(data, subscription):
            filtered_count += 1
            logger.debug(f"‚è≠Ô∏è Filtered out for {ws.client} (doesn't match subscription)")
            continue

        try:
            await ws.send_json(message)
            sent_count += 1
            # Track outbound message
            record_websocket_message("/ws/realtime/", "outbound")
            logger.debug(f"‚úÖ Sent to dashboard: {ws.client}")
        except Exception as e:
            logger.error(f"Failed to send to dashboard: {e}")
            disconnected.append(ws)

    # Remove disconnected clients
    for ws in disconnected:
        connected_dashboards.pop(ws, None)
        logger.info(f"Removed disconnected dashboard: {ws.client}")

    logger.info(f"\nüì° Broadcast Summary:")
    logger.info(f"  Total clients: {len(connected_dashboards) + len(disconnected)}")
    logger.info(f"  Messages sent: {sent_count}")
    logger.info(f"  Filtered out: {filtered_count}")
    logger.info(f"  Disconnected: {len(disconnected)}")
    logger.info("=" * 80)


# ============================================================================
# REST API Endpoints
# ============================================================================

@app.get("/api/dashboard/metrics/")
async def get_dashboard_metrics():
    """
    Get global dashboard metrics with Redis caching
    Cache TTL: 10 seconds
    Compatible with existing frontend API
    """
    # Try cache first
    if cache_manager:
        cached = await cache_manager.get_dashboard_metrics()
        if cached:
            logger.debug("‚úÖ Cache HIT: dashboard_metrics")
            return cached

    logger.debug("‚ùå Cache MISS: dashboard_metrics - querying database")

    async with AsyncSessionLocal() as db:
        # Calculate grade distribution
        grade_distribution = {}
        total_score = 0.0

        result = await db.execute(select(SensorSample).order_by(SensorSample.timestamp.desc()).limit(100))
        samples = result.scalars().all()

        for sample in samples:
            grade = sample.fusion_final_grade
            score = sample.fusion_final_score

            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
            total_score += score

        avg_quality = total_score / len(samples) if samples else 0.0

        # Calculate success rate (non-REJECT)
        reject_count = grade_distribution.get("REJECT", 0)
        success_rate = ((len(samples) - reject_count) / len(samples) * 100) if samples else 0.0

        # Calculate samples by period
        now = datetime.utcnow()
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        week_start = now - timedelta(days=7)
        month_start = now - timedelta(days=30)

        # Count samples by period
        samples_today = await db.execute(
            select(func.count(SensorSample.id)).where(SensorSample.timestamp >= today_start)
        )
        samples_week = await db.execute(
            select(func.count(SensorSample.id)).where(SensorSample.timestamp >= week_start)
        )
        samples_month = await db.execute(
            select(func.count(SensorSample.id)).where(SensorSample.timestamp >= month_start)
        )
        samples_total = await db.execute(
            select(func.count(SensorSample.id))
        )

        metrics = {
            "totalSamples": samples_total.scalar() or 0,
            "samplesProcessedToday": samples_today.scalar() or 0,
            "samplesProcessedWeek": samples_week.scalar() or 0,
            "samplesProcessedMonth": samples_month.scalar() or 0,
            "averageQuality": round(avg_quality, 2),
            "gradeDistribution": grade_distribution,
            "activeAlerts": grade_distribution.get("C", 0) + grade_distribution.get("REJECT", 0),
            "successRate": round(success_rate, 1),
            "activeDevices": 1,
            "trend": 0.0,
            "period": "7days"
        }

        # Cache the result (10 seconds TTL)
        if cache_manager:
            await cache_manager.set_dashboard_metrics(metrics, ttl=10)

        return metrics


@app.get("/api/dashboard/foie-gras-metrics/")
async def get_foie_gras_metrics():
    """
    Get foie gras specific metrics with Redis caching
    Cache TTL: 10 seconds (reduced for real-time updates)

    Returns complete metrics as per DASHBOARD_KPI_DOCUMENTATION.md:
    - Operational metrics (conformity, reject, downgrade rates, cadence)
    - Quality scores (dimensional, color, global)
    - Instant metrics (thickness, L*, Delta E, volume)
    - Process capability (Cp/Cpk)
    - Moving average (10 and 50 samples with trend)
    - Dimensional deviation (mm and %)
    - Maturity index (spectral ratio)
    - Freshness score (with shelf life estimation)
    - Color homogeneity (CV%)
    - Spectral bands (9 channels from 415nm to NIR)
    - Alerts (real-time quality alerts)
    """
    # Try cache first
    if cache_manager:
        cached = await cache_manager.get_foie_gras_metrics()
        if cached:
            logger.debug("‚úÖ Cache HIT: foie_gras_metrics")
            return cached

    logger.debug("‚ùå Cache MISS: foie_gras_metrics - querying database")

    async with AsyncSessionLocal() as db:
        result = await db.execute(select(SensorSample).order_by(SensorSample.timestamp.desc()).limit(100))
        samples = list(result.scalars().all())

        if not samples:
            empty_metrics = {
                "operational": {"conformity_rate": 0.0, "downgrade_rate": 0.0, "reject_rate": 0.0, "control_cadence": 0},
                "quality_scores": {"dimensional_conformity": 0.0, "color_conformity": 0.0, "global_quality_score": 0.0},
                "instant_metrics": {"thickness_mm": 0.0, "l_star": 0.0, "delta_e": 0.0, "volume_mm3": 0.0},
                "process_capability": {"cp": 0.0, "cpk": 0.0, "process_capability": "unknown", "mean": 0.0, "std": 0.0, "is_centered": False},
                "moving_average": {"moving_avg": 0.0, "moving_avg_10": 0.0, "moving_avg_50": 0.0, "trend": "stable", "slope": 0.0},
                "dimensional_deviation": {"deviation_mm": 0.0, "deviation_percent": 0.0, "target_mm": 50.0, "tolerance_mm": 5.0, "is_within_tolerance": True},
                "maturity_index": {"maturity_index": 0.0, "maturity_stage": "unknown", "spectral_ratio_red_nir": 0.0},
                "freshness_score": {"freshness_score": 0.0, "freshness_trend": "unknown", "estimated_shelf_life_hours": 0.0, "spectral_degradation_rate": 0.0},
                "color_homogeneity": {"color_homogeneity_cv": 0.0, "color_uniformity": "unknown", "color_std_delta_e": 0.0, "color_mean_delta_e": 0.0},
                "spectral_bands": {"spectral_bands": {}, "spectral_profile": "unknown", "red_orange_ratio": 0.0, "total_intensity": 0},
                "alerts": [],
                "targets": {"conformity_target": 95, "cadence_target": 120}
            }
            if cache_manager:
                await cache_manager.set_foie_gras_metrics(empty_metrics, ttl=60)
            return empty_metrics

        import statistics
        import math

        # Get latest sample for instant metrics
        latest = samples[0]

        # Calculate operational metrics
        total_score = sum(sample.fusion_final_score for sample in samples)
        avg_score = total_score / len(samples)

        grade_counts = {}
        for sample in samples:
            grade = sample.fusion_final_grade
            grade_counts[grade] = grade_counts.get(grade, 0) + 1

        reject_count = grade_counts.get("REJECT", 0)
        c_count = grade_counts.get("C", 0)
        conformity_count = len(samples) - reject_count
        conformity_rate = (conformity_count / len(samples) * 100)
        reject_rate = (reject_count / len(samples) * 100)
        downgrade_rate = (c_count / len(samples) * 100)

        # Calculate process capability (Cp/Cpk)
        thicknesses = [s.vl53l8ch_avg_height_mm for s in samples if s.vl53l8ch_avg_height_mm]
        USL, LSL, TARGET = 55.0, 45.0, 50.0
        if len(thicknesses) >= 10:
            mean_thickness = statistics.mean(thicknesses)
            std_thickness = statistics.stdev(thicknesses) if len(thicknesses) > 1 else 0.1
            cp = (USL - LSL) / (6 * std_thickness) if std_thickness > 0 else 0
            cpk = min((USL - mean_thickness) / (3 * std_thickness), (mean_thickness - LSL) / (3 * std_thickness)) if std_thickness > 0 else 0
            is_centered = abs(mean_thickness - TARGET) < 1.0
            capability = "capable" if cpk >= 1.33 else ("acceptable" if cpk >= 1.0 else "incapable")
        else:
            mean_thickness, std_thickness, cp, cpk, capability, is_centered = 50.0, 2.0, 1.0, 1.0, "unknown", True

        # Calculate moving averages
        thicknesses_ordered = [s.vl53l8ch_avg_height_mm for s in samples if s.vl53l8ch_avg_height_mm]
        moving_avg = statistics.mean(thicknesses_ordered) if thicknesses_ordered else 50.0
        moving_avg_10 = statistics.mean(thicknesses_ordered[:10]) if len(thicknesses_ordered) >= 10 else moving_avg
        moving_avg_50 = statistics.mean(thicknesses_ordered[:50]) if len(thicknesses_ordered) >= 50 else moving_avg

        # Calculate trend (simple linear regression slope)
        if len(thicknesses_ordered) >= 10:
            x_values = list(range(len(thicknesses_ordered[:50])))
            y_values = thicknesses_ordered[:50]
            n = len(x_values)
            slope = (n * sum(x * y for x, y in zip(x_values, y_values)) - sum(x_values) * sum(y_values)) / (n * sum(x**2 for x in x_values) - sum(x_values)**2) if n > 1 else 0
            trend = "rising" if slope > 0.1 else ("falling" if slope < -0.1 else "stable")
        else:
            slope, trend = 0.0, "stable"

        # Calculate dimensional deviation
        deviation_mm = mean_thickness - TARGET
        deviation_percent = (deviation_mm / TARGET) * 100
        is_within_tolerance = abs(deviation_mm) <= 5.0

        # Calculate maturity index (spectral ratio Red/NIR)
        if latest.as7341_channels:
            red_intensity = latest.as7341_channels.get('F8_680nm', 3900)
            nir_intensity = latest.as7341_channels.get('NIR', 2800)
            maturity_index = red_intensity / nir_intensity if nir_intensity > 0 else 0
            maturity_stage = "optimal" if maturity_index > 2.0 else ("mature" if maturity_index > 1.5 else ("immature" if maturity_index > 1.0 else "out_of_spec"))
        else:
            maturity_index, maturity_stage = 1.85, "optimal"

        # Calculate freshness score (0-100 based on spectral deviation)
        spectral_channels = latest.as7341_channels if latest.as7341_channels else {}
        if spectral_channels:
            reference_spectrum = [1200, 1500, 2000, 2400, 3100, 3700, 4100, 3800, 2700]
            actual_spectrum = [
                spectral_channels.get('F1_415nm', 1250),
                spectral_channels.get('F2_445nm', 1580),
                spectral_channels.get('F3_480nm', 2100),
                spectral_channels.get('F4_515nm', 2450),
                spectral_channels.get('F5_555nm', 3200),
                spectral_channels.get('F6_590nm', 3800),
                spectral_channels.get('F7_630nm', 4200),
                spectral_channels.get('F8_680nm', 3900),
                spectral_channels.get('NIR', 2800)
            ]
            spectral_deviation = sum(abs(a - r) for a, r in zip(actual_spectrum, reference_spectrum)) / len(reference_spectrum)
            max_deviation = 500
            freshness_score = max(0, 100 * (1 - spectral_deviation / max_deviation))
            freshness_trend = "stable"
            estimated_shelf_life_hours = 48 if freshness_score > 90 else (24 if freshness_score > 70 else 12)
            spectral_degradation_rate = spectral_deviation / 1000
        else:
            freshness_score, freshness_trend, estimated_shelf_life_hours, spectral_degradation_rate = 92.0, "stable", 48.0, 0.02

        # Calculate color homogeneity (CV% of Delta E across samples)
        delta_e_values = [s.as7341_color_analysis.get('delta_e', 2.0) if s.as7341_color_analysis else 2.0 for s in samples[:50]]
        if len(delta_e_values) >= 10:
            mean_delta_e = statistics.mean(delta_e_values)
            std_delta_e = statistics.stdev(delta_e_values) if len(delta_e_values) > 1 else 0.1
            color_homogeneity_cv = (std_delta_e / mean_delta_e * 100) if mean_delta_e > 0 else 0
            color_uniformity = "excellent" if color_homogeneity_cv < 5 else ("good" if color_homogeneity_cv < 10 else ("acceptable" if color_homogeneity_cv < 20 else "low"))
        else:
            mean_delta_e, std_delta_e, color_homogeneity_cv, color_uniformity = 2.1, 1.5, 3.2, "excellent"

        # Spectral bands (9 channels)
        spectral_bands_data = spectral_channels if spectral_channels else {
            'F1_415nm': 1250, 'F2_445nm': 1580, 'F3_480nm': 2100, 'F4_515nm': 2450,
            'F5_555nm': 3200, 'F6_590nm': 3800, 'F7_630nm': 4200, 'F8_680nm': 3900, 'NIR': 2800
        }
        spectral_bands = {
            "415nm_violet": spectral_bands_data.get('F1_415nm', 1250),
            "445nm_indigo": spectral_bands_data.get('F2_445nm', 1580),
            "480nm_blue": spectral_bands_data.get('F3_480nm', 2100),
            "515nm_cyan": spectral_bands_data.get('F4_515nm', 2450),
            "555nm_green": spectral_bands_data.get('F5_555nm', 3200),
            "590nm_yellow": spectral_bands_data.get('F6_590nm', 3800),
            "630nm_orange": spectral_bands_data.get('F7_630nm', 4200),
            "680nm_red": spectral_bands_data.get('F8_680nm', 3900),
            "nir_850nm": spectral_bands_data.get('NIR', 2800)
        }
        total_intensity = sum(spectral_bands.values())
        red_orange_ratio = spectral_bands["680nm_red"] / spectral_bands["630nm_orange"] if spectral_bands["630nm_orange"] > 0 else 1.0
        spectral_profile = "foie_gras_cru_extra"

        # Generate alerts
        alerts = []
        if cpk < 1.0:
            alerts.append({"type": "process_capability", "severity": "high", "message": f"Capabilit√© process insuffisante (Cpk={cpk:.2f})"})
        if latest.as7341_color_analysis and latest.as7341_color_analysis.get('delta_e', 0) > 8.0:
            alerts.append({"type": "color_deviation", "severity": "critical", "message": "D√©viation couleur critique (ŒîE > 8.0)"})
        if abs(deviation_mm) > 5.0:
            alerts.append({"type": "dimensional", "severity": "high", "message": f"Sous-remplissage d√©tect√© ({deviation_mm:+.1f} mm)"})
        if maturity_index < 1.0:
            alerts.append({"type": "maturity", "severity": "high", "message": "Tendance oxydation d√©tect√©e"})
        if latest.fusion_final_grade in ["C", "REJECT"]:
            alerts.append({"type": "quality", "severity": "high" if latest.fusion_final_grade == "REJECT" else "medium", "message": f"Grade {latest.fusion_final_grade} d√©tect√©"})

        metrics = {
            "operational": {
                "conformity_rate": round(conformity_rate, 1),
                "downgrade_rate": round(downgrade_rate, 1),
                "reject_rate": round(reject_rate, 1),
                "control_cadence": len(samples)
            },
            "quality_scores": {
                "dimensional_conformity": round(latest.vl53l8ch_quality_score * 100 if latest.vl53l8ch_quality_score else 0, 1),
                "color_conformity": round(latest.as7341_quality_score * 100 if latest.as7341_quality_score else 0, 1),
                "global_quality_score": round(avg_score * 100, 1)
            },
            "instant_metrics": {
                "thickness_mm": round(latest.vl53l8ch_avg_height_mm if latest.vl53l8ch_avg_height_mm else 0, 2),
                "l_star": round(latest.as7341_color_analysis.get('l_star', 0.0) if latest.as7341_color_analysis else 0.0, 2),
                "delta_e": round(latest.as7341_color_analysis.get('delta_e', 0.0) if latest.as7341_color_analysis else 0.0, 2),
                "volume_mm3": round(latest.vl53l8ch_volume_mm3 if latest.vl53l8ch_volume_mm3 else 0, 2)
            },
            "process_capability": {
                "cp": round(cp, 2),
                "cpk": round(cpk, 2),
                "process_capability": capability,
                "mean": round(mean_thickness, 1),
                "std": round(std_thickness, 2),
                "is_centered": is_centered
            },
            "moving_average": {
                "moving_avg": round(moving_avg, 2),
                "moving_avg_10": round(moving_avg_10, 2),
                "moving_avg_50": round(moving_avg_50, 2),
                "trend": trend,
                "slope": round(slope, 3)
            },
            "dimensional_deviation": {
                "deviation_mm": round(deviation_mm, 2),
                "deviation_percent": round(deviation_percent, 1),
                "target_mm": TARGET,
                "tolerance_mm": 5.0,
                "is_within_tolerance": is_within_tolerance
            },
            "maturity_index": {
                "maturity_index": round(maturity_index, 2),
                "maturity_stage": maturity_stage,
                "spectral_ratio_red_nir": round(maturity_index, 2)
            },
            "freshness_score": {
                "freshness_score": round(freshness_score, 1),
                "freshness_trend": freshness_trend,
                "estimated_shelf_life_hours": estimated_shelf_life_hours,
                "spectral_degradation_rate": round(spectral_degradation_rate, 3)
            },
            "color_homogeneity": {
                "color_homogeneity_cv": round(color_homogeneity_cv, 1),
                "color_uniformity": color_uniformity,
                "color_std_delta_e": round(std_delta_e, 2),
                "color_mean_delta_e": round(mean_delta_e, 2)
            },
            "spectral_bands": {
                "spectral_bands": spectral_bands,
                "spectral_profile": spectral_profile,
                "red_orange_ratio": round(red_orange_ratio, 2),
                "total_intensity": total_intensity
            },
            "alerts": alerts,
            "targets": {
                "conformity_target": 95,
                "cadence_target": 120
            }
        }

        # Cache the result (10 sec TTL for real-time updates)
        if cache_manager:
            await cache_manager.set_foie_gras_metrics(metrics, ttl=10)

        return metrics


@app.get("/api/dashboard/foie-gras-alerts/")
async def get_foie_gras_alerts():
    """
    Get active alerts for foie gras production
    """
    async with AsyncSessionLocal() as db:
        alerts = []

        result = await db.execute(select(SensorSample).order_by(SensorSample.timestamp.desc()).limit(100))
        samples = result.scalars().all()

        # Generate alerts from recent samples
        for sample in samples:
            grade = sample.fusion_final_grade

            if grade in ["C", "REJECT"]:
                alerts.append({
                    "id": sample.sample_id,
                    "type": "quality",
                    "severity": "high" if grade == "REJECT" else "medium",
                    "message": f"√âchantillon {grade} d√©tect√©",
                    "timestamp": sample.timestamp.isoformat(),
                    "sample_id": sample.sample_id
                })

        return alerts


@app.get("/api/cache/stats")
async def get_cache_stats():
    """
    Get Redis cache statistics for monitoring

    Returns:
        - Cache status (connected/unavailable)
        - Hit/miss statistics
        - Key count
        - Memory usage
        - Uptime
    """
    if not cache_manager:
        return {
            "status": "unavailable",
            "message": "Cache manager not initialized"
        }

    stats = await cache_manager.get_stats()
    return stats


@app.get("/api/analytics/hourly")
async def get_hourly_analytics(hours: int = 24):
    """
    Get hourly analytics using TimescaleDB continuous aggregates
    ULTRA-FAST: Uses pre-computed aggregates (sensor_data_hourly)

    Performance: ~10-50ms (vs 500ms+ for raw query)

    Args:
        hours: Number of hours to fetch (default 24, max 168 = 7 days)
    """
    # Try cache first
    cache_key = f"analytics_hourly_{hours}"
    if cache_manager:
        cached = await cache_manager.redis.get(cache_key) if cache_manager._is_available() else None
        if cached:
            logger.debug(f"‚úÖ Cache HIT: {cache_key}")
            return json.loads(cached)

    logger.debug(f"‚ùå Cache MISS: {cache_key} - querying continuous aggregate")

    # Limit hours to prevent abuse
    hours = min(hours, 168)  # Max 7 days

    async with AsyncSessionLocal() as db:
        # Query the continuous aggregate (pre-computed, FAST!)
        query = f"""
        SELECT
            bucket,
            device_id,
            sample_count,
            avg_quality,
            min_quality,
            max_quality,
            grade_a_plus,
            grade_a,
            grade_b,
            grade_c,
            grade_reject
        FROM sensor_data_hourly
        WHERE bucket >= NOW() - INTERVAL '{hours} hours'
        ORDER BY bucket DESC
        """

        result = await db.execute(query)
        rows = result.fetchall()

        # Format response
        analytics = []
        for row in rows:
            analytics.append({
                "timestamp": row[0].isoformat(),
                "device_id": row[1],
                "sample_count": row[2],
                "avg_quality": round(float(row[3]), 3) if row[3] else 0.0,
                "min_quality": round(float(row[4]), 3) if row[4] else 0.0,
                "max_quality": round(float(row[5]), 3) if row[5] else 0.0,
                "grades": {
                    "A+": row[6],
                    "A": row[7],
                    "B": row[8],
                    "C": row[9],
                    "REJECT": row[10]
                }
            })

        # Cache for 10 minutes
        if cache_manager and cache_manager._is_available():
            await cache_manager.redis.setex(
                cache_key,
                600,  # 10 min TTL
                json.dumps(analytics, default=str)
            )

        return analytics


@app.get("/api/analytics/daily")
async def get_daily_analytics(days: int = 7):
    """
    Get daily analytics using TimescaleDB continuous aggregates
    ULTRA-FAST: Uses pre-computed aggregates (sensor_data_daily)

    Performance: ~5-30ms (vs 1000ms+ for raw query)

    Args:
        days: Number of days to fetch (default 7, max 90)
    """
    # Try cache first
    cache_key = f"analytics_daily_{days}"
    if cache_manager:
        cached = await cache_manager.redis.get(cache_key) if cache_manager._is_available() else None
        if cached:
            logger.debug(f"‚úÖ Cache HIT: {cache_key}")
            return json.loads(cached)

    logger.debug(f"‚ùå Cache MISS: {cache_key} - querying continuous aggregate")

    # Limit days to prevent abuse
    days = min(days, 90)  # Max 90 days

    async with AsyncSessionLocal() as db:
        # Query the continuous aggregate (pre-computed, FAST!)
        query = f"""
        SELECT
            bucket,
            device_id,
            sample_count,
            avg_quality,
            defect_count
        FROM sensor_data_daily
        WHERE bucket >= NOW() - INTERVAL '{days} days'
        ORDER BY bucket DESC
        """

        result = await db.execute(query)
        rows = result.fetchall()

        # Format response
        analytics = []
        for row in rows:
            analytics.append({
                "date": row[0].isoformat(),
                "device_id": row[1],
                "sample_count": row[2],
                "avg_quality": round(float(row[3]), 3) if row[3] else 0.0,
                "defect_count": row[4],
                "conformity_rate": round((1 - (row[4] / row[2])) * 100, 1) if row[2] > 0 else 0.0
            })

        # Cache for 30 minutes
        if cache_manager and cache_manager._is_available():
            await cache_manager.redis.setex(
                cache_key,
                1800,  # 30 min TTL
                json.dumps(analytics, default=str)
            )

        return analytics


@app.get("/api/analytics/summary")
async def get_analytics_summary():
    """
    Get quick summary statistics using continuous aggregates
    Perfect for dashboard widgets

    Performance: ~20-100ms (uses hourly aggregates)
    """
    # Try cache first
    if cache_manager:
        cached = await cache_manager.redis.get("analytics_summary") if cache_manager._is_available() else None
        if cached:
            logger.debug("‚úÖ Cache HIT: analytics_summary")
            return json.loads(cached)

    logger.debug("‚ùå Cache MISS: analytics_summary - computing from aggregates")

    async with AsyncSessionLocal() as db:
        # Last 24 hours from hourly aggregate
        query = """
        SELECT
            SUM(sample_count) as total_samples,
            AVG(avg_quality) as overall_quality,
            SUM(grade_a_plus + grade_a) as conforming,
            SUM(grade_c + grade_reject) as defects
        FROM sensor_data_hourly
        WHERE bucket >= NOW() - INTERVAL '24 hours'
        """

        result = await db.execute(query)
        row = result.fetchone()

        total = row[0] or 0
        quality = row[1] or 0.0
        conforming = row[2] or 0
        defects = row[3] or 0

        summary = {
            "period": "24h",
            "total_samples": int(total),
            "avg_quality": round(float(quality), 3),
            "conformity_rate": round((conforming / total * 100), 1) if total > 0 else 0.0,
            "defect_rate": round((defects / total * 100), 1) if total > 0 else 0.0,
            "status": "excellent" if quality >= 0.9 else "good" if quality >= 0.8 else "warning"
        }

        # Cache for 2 minutes
        if cache_manager and cache_manager._is_available():
            await cache_manager.redis.setex(
                "analytics_summary",
                120,  # 2 min TTL
                json.dumps(summary, default=str)
            )

        return summary


# ============================================================================
# PHASE 4 - UX: Trend Analysis Endpoints
# ============================================================================

@app.get("/api/analysis/trends/{device_id}")
async def get_device_trends(
    device_id: str,
    metric: str = "quality_score",
    time_range: str = "24h"
):
    """
    Get historical trend data for visualization

    Args:
        device_id: Device ID to analyze
        metric: Metric to plot (quality_score, volume, freshness, etc.)
        time_range: Time range (1h, 24h, 7d, 30d)

    Returns:
        Array of {timestamp, value, avg, min, max} for charting

    Performance: 10-50ms (uses continuous aggregates + cache)
    """
    # Map time_range to hours
    range_map = {"1h": 1, "24h": 24, "7d": 168, "30d": 720}
    hours = range_map.get(time_range, 24)

    # Try cache first
    cache_key = f"trends_{device_id}_{metric}_{time_range}"
    if cache_manager:
        cached = await cache_manager.redis.get(cache_key) if cache_manager._is_available() else None
        if cached:
            logger.debug(f"‚úÖ Cache HIT: {cache_key}")
            return json.loads(cached)

    logger.debug(f"‚ùå Cache MISS: {cache_key} - querying aggregates")

    async with AsyncSessionLocal() as db:
        # Map metric to database column
        metric_map = {
            "quality_score": "avg_quality",
            "volume": "AVG(vl53l8ch_volume_mm3)",
            "freshness": "AVG(as7341_freshness_index)",
            "height": "AVG(vl53l8ch_avg_height_mm)"
        }

        # Use hourly aggregates for speed
        if hours <= 168:  # <= 7 days
            if metric == "quality_score":
                query = f"""
                SELECT
                    bucket as timestamp,
                    avg_quality as value,
                    avg_quality as avg,
                    min_quality as min,
                    max_quality as max
                FROM sensor_data_hourly
                WHERE device_id = '{device_id}'
                  AND bucket >= NOW() - INTERVAL '{hours} hours'
                ORDER BY bucket ASC
                """
            else:
                # Need to query raw data for other metrics
                query = f"""
                SELECT
                    time_bucket('1 hour', timestamp) as timestamp,
                    {metric_map.get(metric, 'AVG(fusion_final_score)')} as value,
                    {metric_map.get(metric, 'AVG(fusion_final_score)')} as avg,
                    MIN(fusion_final_score) as min,
                    MAX(fusion_final_score) as max
                FROM sensor_samples
                WHERE device_id = '{device_id}'
                  AND timestamp >= NOW() - INTERVAL '{hours} hours'
                GROUP BY time_bucket('1 hour', timestamp)
                ORDER BY timestamp ASC
                """
        else:
            # Use daily aggregates for longer periods
            query = f"""
            SELECT
                bucket as timestamp,
                avg_quality as value,
                avg_quality as avg,
                avg_quality as min,
                avg_quality as max
            FROM sensor_data_daily
            WHERE device_id = '{device_id}'
              AND bucket >= NOW() - INTERVAL '{hours} hours'
            ORDER BY bucket ASC
            """

        result = await db.execute(query)
        rows = result.fetchall()

        # Format response for charts
        trend_data = []
        for row in rows:
            trend_data.append({
                "timestamp": row[0].isoformat(),
                "value": round(float(row[1]), 3) if row[1] else 0.0,
                "avg": round(float(row[2]), 3) if row[2] else 0.0,
                "min": round(float(row[3]), 3) if row[3] else 0.0,
                "max": round(float(row[4]), 3) if row[4] else 0.0
            })

        # Cache for 5 minutes
        if cache_manager and cache_manager._is_available():
            await cache_manager.redis.setex(
                cache_key,
                300,  # 5 min TTL
                json.dumps(trend_data, default=str)
            )

        return trend_data


@app.get("/api/analysis/compare")
async def compare_samples(sample_ids: str):
    """
    Compare multiple samples side-by-side

    Args:
        sample_ids: Comma-separated sample IDs (e.g., "id1,id2,id3")

    Returns:
        Array of sample data for comparison

    Example: /api/analysis/compare?sample_ids=sample-001,sample-002,sample-003
    """
    ids = sample_ids.split(",")
    if len(ids) > 5:
        return {"error": "Maximum 5 samples for comparison"}

    async with AsyncSessionLocal() as db:
        # Fetch samples
        query = f"""
        SELECT
            sample_id,
            timestamp,
            device_id,
            fusion_final_score,
            fusion_final_grade,
            vl53l8ch_volume_mm3,
            vl53l8ch_avg_height_mm,
            vl53l8ch_quality_score,
            as7341_quality_score,
            as7341_freshness_index,
            as7341_fat_quality_index
        FROM sensor_samples
        WHERE sample_id IN ({','.join(f"'{id}'" for id in ids)})
        """

        result = await db.execute(query)
        rows = result.fetchall()

        # Format for comparison
        comparison = []
        for row in rows:
            comparison.append({
                "sample_id": row[0],
                "timestamp": row[1].isoformat(),
                "device_id": row[2],
                "fusion": {
                    "final_score": round(float(row[3]), 3) if row[3] else 0.0,
                    "final_grade": row[4]
                },
                "vl53l8ch": {
                    "volume_mm3": round(float(row[5]), 2) if row[5] else 0.0,
                    "avg_height_mm": round(float(row[6]), 2) if row[6] else 0.0,
                    "quality_score": round(float(row[7]), 3) if row[7] else 0.0
                },
                "as7341": {
                    "quality_score": round(float(row[8]), 3) if row[8] else 0.0,
                    "freshness_index": round(float(row[9]), 3) if row[9] else 0.0,
                    "fat_quality_index": round(float(row[10]), 3) if row[10] else 0.0
                }
            })

        return comparison


# ============================================================================
# PHASE 4 - UX: Alert System
# ============================================================================

# In-memory alert rules storage (TODO: Move to database)
alert_rules = []

@app.post("/api/alerts/rules")
async def create_alert_rule(rule: dict):
    """
    Create a new alert rule

    Request body:
    {
        "name": "Low Quality Alert",
        "condition": "quality_score < 0.7",
        "severity": "warning",  // info, warning, critical
        "enabled": true,
        "notify_channels": ["websocket", "email"]
    }
    """
    rule_id = f"rule-{len(alert_rules) + 1}"
    alert_rule = {
        "id": rule_id,
        "name": rule.get("name"),
        "condition": rule.get("condition"),
        "severity": rule.get("severity", "warning"),
        "enabled": rule.get("enabled", True),
        "notify_channels": rule.get("notify_channels", ["websocket"]),
        "created_at": datetime.utcnow().isoformat()
    }

    alert_rules.append(alert_rule)

    logger.info(f"‚úÖ Alert rule created: {alert_rule['name']}")

    return {
        "status": "created",
        "rule": alert_rule
    }


@app.get("/api/alerts/rules")
async def get_alert_rules():
    """Get all alert rules"""
    return alert_rules


@app.delete("/api/alerts/rules/{rule_id}")
async def delete_alert_rule(rule_id: str):
    """Delete an alert rule"""
    global alert_rules
    alert_rules = [r for r in alert_rules if r["id"] != rule_id]

    return {"status": "deleted", "rule_id": rule_id}


@app.put("/api/alerts/rules/{rule_id}")
async def update_alert_rule(rule_id: str, updates: dict):
    """Update an alert rule"""
    for rule in alert_rules:
        if rule["id"] == rule_id:
            rule.update(updates)
            rule["updated_at"] = datetime.utcnow().isoformat()
            return {"status": "updated", "rule": rule}

    return {"error": "Rule not found"}, 404


def evaluate_alert_rules(sample_data: dict) -> list:
    """
    Evaluate all enabled alert rules against sample data

    Returns list of triggered alerts
    """
    triggered_alerts = []

    # Extract values for evaluation
    fusion = sample_data.get("fusion", {})
    vl53l8ch = sample_data.get("vl53l8ch") or sample_data.get("VL53L8CH", {})

    # Create evaluation context
    context = {
        "quality_score": fusion.get("final_score", 0),
        "grade": fusion.get("final_grade", ""),
        "volume_mm3": vl53l8ch.get("volume_mm3", 0),
        "height_mm": vl53l8ch.get("avg_height_mm", 0)
    }

    for rule in alert_rules:
        if not rule.get("enabled"):
            continue

        try:
            # Simple condition evaluation
            condition = rule["condition"]

            # Replace variables
            eval_str = condition
            for key, value in context.items():
                eval_str = eval_str.replace(key, str(value))

            # Evaluate (Note: eval is dangerous in production, use a proper parser)
            if eval(eval_str):
                triggered_alerts.append({
                    "rule_id": rule["id"],
                    "rule_name": rule["name"],
                    "severity": rule["severity"],
                    "message": f"{rule['name']}: {condition}",
                    "timestamp": datetime.utcnow().isoformat(),
                    "sample_id": sample_data.get("sample_id"),
                    "device_id": sample_data.get("device_id")
                })

                logger.warning(f"üö® Alert triggered: {rule['name']} for sample {sample_data.get('sample_id')}")

        except Exception as e:
            logger.error(f"Error evaluating rule {rule['id']}: {e}")

    return triggered_alerts


# ============================================================================
# PHASE 4 - UX: Export Data Endpoints
# ============================================================================

@app.get("/api/export/samples")
async def export_samples(
    format: str = "csv",
    start_date: str = None,
    end_date: str = None,
    device_id: str = None
):
    """
    Export sample data in various formats

    Args:
        format: Output format (csv, json)
        start_date: Start date (ISO format)
        end_date: End date (ISO format)
        device_id: Filter by device ID

    Returns:
        File download response

    Formats:
        - csv: Excel-compatible CSV
        - json: Raw JSON data
        - (TODO: excel, pdf)
    """
    from io import StringIO
    import csv
    from fastapi.responses import StreamingResponse

    # Build query
    where_clauses = []
    if start_date:
        where_clauses.append(f"timestamp >= '{start_date}'")
    if end_date:
        where_clauses.append(f"timestamp <= '{end_date}'")
    if device_id:
        where_clauses.append(f"device_id = '{device_id}'")

    where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"

    async with AsyncSessionLocal() as db:
        query = f"""
        SELECT
            sample_id,
            timestamp,
            device_id,
            fusion_final_score,
            fusion_final_grade,
            vl53l8ch_volume_mm3,
            vl53l8ch_avg_height_mm,
            vl53l8ch_quality_score,
            as7341_quality_score,
            as7341_freshness_index
        FROM sensor_samples
        WHERE {where_sql}
        ORDER BY timestamp DESC
        LIMIT 10000
        """

        result = await db.execute(query)
        rows = result.fetchall()

        if format == "csv":
            # Generate CSV
            output = StringIO()
            writer = csv.writer(output)

            # Header
            writer.writerow([
                "Sample ID", "Timestamp", "Device ID",
                "Final Score", "Final Grade",
                "Volume (mm¬≥)", "Height (mm)",
                "VL53L8CH Score", "AS7341 Score", "Freshness Index"
            ])

            # Data
            for row in rows:
                writer.writerow([
                    row[0], row[1], row[2], row[3], row[4],
                    row[5], row[6], row[7], row[8], row[9]
                ])

            output.seek(0)

            return StreamingResponse(
                iter([output.getvalue()]),
                media_type="text/csv",
                headers={
                    "Content-Disposition": f"attachment; filename=sqal_export_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv"
                }
            )

        elif format == "json":
            # Return JSON array
            data = []
            for row in rows:
                data.append({
                    "sample_id": row[0],
                    "timestamp": row[1].isoformat(),
                    "device_id": row[2],
                    "final_score": float(row[3]) if row[3] else 0,
                    "final_grade": row[4],
                    "volume_mm3": float(row[5]) if row[5] else 0,
                    "height_mm": float(row[6]) if row[6] else 0,
                    "vl53l8ch_score": float(row[7]) if row[7] else 0,
                    "as7341_score": float(row[8]) if row[8] else 0,
                    "freshness_index": float(row[9]) if row[9] else 0
                })

            return data

        else:
            return {"error": f"Unsupported format: {format}"}


@app.get("/api/sensors/vl53l8ch/raw/")
async def get_vl53l8ch_raw(limit: int = 10):
    """
    Get latest VL53L8CH (ToF) raw sensor data
    
    Args:
        limit: Number of samples to return (default 10, max 100)
    
    Returns:
        {
            "results": [
                {
                    "sample_id": "...",
                    "timestamp": "...",
                    "device_id": "...",
                    "distances": [64 values],  // Flattened 8x8 matrix
                    "distance_matrix": [[8x8]],
                    "reflectance_matrix": [[8x8]],
                    "amplitude_matrix": [[8x8]]
                }
            ]
        }
    """
    try:
        limit = min(limit, 100)  # Cap at 100
        
        async with AsyncSessionLocal() as db:
            # Query latest samples with VL53L8CH data
            result = await db.execute(
                select(SensorSample)
                .where(SensorSample.vl53l8ch_distance_matrix.isnot(None))
                .order_by(SensorSample.timestamp.desc())
                .limit(limit)
            )
            samples = result.scalars().all()
            
            # Format response
            results = []
            for sample in samples:
                # Flatten distance matrix to 1D array
                distances = []
                if sample.vl53l8ch_distance_matrix:
                    for row in sample.vl53l8ch_distance_matrix:
                        distances.extend(row)
                
                results.append({
                    "sample_id": sample.sample_id,
                    "timestamp": sample.timestamp.isoformat(),
                    "device_id": sample.device_id,
                    "distances": distances,
                    "distance_matrix": sample.vl53l8ch_distance_matrix,
                    "reflectance_matrix": sample.vl53l8ch_reflectance_matrix,
                    "amplitude_matrix": sample.vl53l8ch_amplitude_matrix
                })
            
            return {"results": results}
            
    except Exception as e:
        logger.error(f"Error fetching VL53L8CH raw data: {e}")
        return {"results": []}


@app.get("/api/sensors/vl53l8ch/analysis/")
async def get_vl53l8ch_analysis(limit: int = 50):
    """
    Get latest VL53L8CH analysis data
    
    Args:
        limit: Number of samples to return (default 50, max 200)
    
    Returns:
        {
            "results": [
                {
                    "sample_id": "...",
                    "timestamp": "...",
                    "grade": "A+",
                    "quality_score": 0.95,
                    "volume_mm3": 1234.5,
                    "avg_height_mm": 45.2,
                    "surface_uniformity": 0.92,
                    "defects": [...]
                }
            ]
        }
    """
    try:
        limit = min(limit, 200)  # Cap at 200
        
        async with AsyncSessionLocal() as db:
            # Query latest samples with analysis data
            result = await db.execute(
                select(SensorSample)
                .where(SensorSample.vl53l8ch_grade.isnot(None))
                .order_by(SensorSample.timestamp.desc())
                .limit(limit)
            )
            samples = result.scalars().all()
            
            # Format response
            results = []
            for sample in samples:
                results.append({
                    "sample_id": sample.sample_id,
                    "timestamp": sample.timestamp.isoformat(),
                    "device_id": sample.device_id,
                    "grade": sample.vl53l8ch_grade,
                    "quality_score": sample.vl53l8ch_quality_score or 0.0,
                    "volume_mm3": sample.vl53l8ch_volume_mm3 or 0.0,
                    "avg_height_mm": sample.vl53l8ch_avg_height_mm or 0.0,
                    "surface_uniformity": sample.vl53l8ch_surface_uniformity or 0.0,
                    "defects": sample.vl53l8ch_defects or []
                })
            
            return {"results": results}
            
    except Exception as e:
        logger.error(f"Error fetching VL53L8CH analysis: {e}")
        return {"results": []}


@app.get("/api/sensors/as7341/raw/")
async def get_as7341_raw(limit: int = 10):
    """
    Get latest AS7341 (Spectral) raw sensor data
    
    Args:
        limit: Number of samples to return (default 10, max 100)
    
    Returns:
        {
            "results": [
                {
                    "sample_id": "...",
                    "timestamp": "...",
                    "device_id": "...",
                    "raw_counts": {
                        "F1_415nm": 1234,
                        "F2_445nm": 2345,
                        ...
                    }
                }
            ]
        }
    """
    try:
        limit = min(limit, 100)  # Cap at 100
        
        async with AsyncSessionLocal() as db:
            # Query latest samples with AS7341 data
            result = await db.execute(
                select(SensorSample)
                .where(SensorSample.as7341_raw_counts.isnot(None))
                .order_by(SensorSample.timestamp.desc())
                .limit(limit)
            )
            samples = result.scalars().all()
            
            # Format response
            results = []
            for sample in samples:
                results.append({
                    "sample_id": sample.sample_id,
                    "timestamp": sample.timestamp.isoformat(),
                    "device_id": sample.device_id,
                    "raw_counts": sample.as7341_raw_counts
                })
            
            return {"results": results}
            
    except Exception as e:
        logger.error(f"Error fetching AS7341 raw data: {e}")
        return {"results": []}


@app.get("/api/sensors/as7341/analysis/")
async def get_as7341_analysis(limit: int = 50):
    """
    Get latest AS7341 analysis data
    
    Args:
        limit: Number of samples to return (default 50, max 200)
    
    Returns:
        {
            "results": [
                {
                    "sample_id": "...",
                    "timestamp": "...",
                    "grade": "A+",
                    "quality_score": 0.95,
                    "color_score": 0.92,
                    "freshness_index": 0.88,
                    "oxidation_level": 0.15
                }
            ]
        }
    """
    try:
        limit = min(limit, 200)  # Cap at 200
        
        async with AsyncSessionLocal() as db:
            # Query latest samples with analysis data
            result = await db.execute(
                select(SensorSample)
                .where(SensorSample.as7341_grade.isnot(None))
                .order_by(SensorSample.timestamp.desc())
                .limit(limit)
            )
            samples = result.scalars().all()
            
            # Format response
            results = []
            for sample in samples:
                results.append({
                    "sample_id": sample.sample_id,
                    "timestamp": sample.timestamp.isoformat(),
                    "device_id": sample.device_id,
                    "grade": sample.as7341_grade,
                    "quality_score": sample.as7341_quality_score or 0.0,
                    "color_score": sample.as7341_color_score or 0.0,
                    "freshness_index": sample.as7341_freshness_index or 0.0,
                    "oxidation_level": sample.as7341_oxidation_level or 0.0
                })
            
            return {"results": results}
            
    except Exception as e:
        logger.error(f"Error fetching AS7341 analysis: {e}")
        return {"results": []}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
