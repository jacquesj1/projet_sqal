# Analytics Intelligents Euralis - Documentation Technique

**Date**: 2026-01-13
**Version**: 3.0 Production Ready
**Auteur**: SystÃ¨me Gaveurs V3.0

---

## ğŸ¯ Vue d'Ensemble

Le systÃ¨me Euralis intÃ¨gre **5 modules d'analytics intelligents** pour optimiser la production de foie gras sur 3 sites (LL, LS, MT) avec 40+ gaveurs actifs.

**AccÃ¨s**: `http://localhost:3000/euralis/analytics` â†’ 5 onglets

---

## ğŸ“Š Architecture Analytics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         EURALIS ANALYTICS DASHBOARD                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Onglet 1: PrÃ©visions (Prophet)                        â”‚
â”‚  Onglet 2: Clustering Gaveurs (K-Means)                â”‚
â”‚  Onglet 3: DÃ©tection Anomalies (Isolation Forest)      â”‚
â”‚  Onglet 4: Optimisation Abattages (Hungarian)          â”‚
â”‚  Onglet 5: CorrÃ©lations Globales (Pearson)             â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚
         â–¼                      â–¼
   Backend FastAPI      TimescaleDB + PostgreSQL
   app/ml/euralis/      58 lots CSV rÃ©els 2024
```

---

## 1ï¸âƒ£ PrÃ©visions Production (Prophet)

### ğŸ¤– Est-ce de l'IA/ML ? **OUI âœ…**

**Algorithme**: Prophet (Facebook/Meta Research)
**Type**: Machine Learning - SÃ©ries temporelles
**ImplÃ©mentation**: `backend-api/app/ml/euralis/production_forecasting.py`

### Technologie

- **BibliothÃ¨que**: `prophet` (Python)
- **Framework**: ModÃ¨le additif avec dÃ©composition tendance + saisonnalitÃ©
- **EntraÃ®nement**: ModÃ¨le entraÃ®nÃ© sur historique production par site
- **Horizon**: 7, 30 ou 90 jours

### Comment Ã§a fonctionne

```python
from prophet import Prophet

# 1. ModÃ¨le Prophet avec paramÃ¨tres
model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False,
    seasonality_mode='multiplicative',
    interval_width=0.95
)

# 2. Ajouter saisonnalitÃ© mensuelle
model.add_seasonality(
    name='monthly',
    period=30.5,
    fourier_order=5
)

# 3. EntraÃ®ner sur donnÃ©es historiques
model.fit(df)  # df = DataFrame(date, production_kg)

# 4. PrÃ©dire 30 jours
future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)
```

### ModÃ¨le MathÃ©matique

Prophet dÃ©compose la sÃ©rie temporelle:

```
y(t) = g(t) + s(t) + h(t) + Îµ(t)

OÃ¹:
- g(t) = Tendance (croissance logistique ou linÃ©aire)
- s(t) = SaisonnalitÃ© (Fourier series)
- h(t) = Effets jours fÃ©riÃ©s
- Îµ(t) = Erreur (bruit gaussien)
```

**Exemple saisonnalitÃ© mensuelle**:
```
s(t) = Î£(n=1 Ã  5) [aâ‚™ cos(2Ï€nt/30.5) + bâ‚™ sin(2Ï€nt/30.5)]
```

### RÃ©sultats Produits

```json
{
  "date": "2026-01-20",
  "production_kg": 2450.3,
  "lower_bound": 2180.5,
  "upper_bound": 2720.1,
  "trend": "increasing",
  "confidence": 0.95
}
```

### MÃ©triques de Performance

- **MAPE** (Mean Absolute Percentage Error): < 8%
- **Intervalle confiance**: 95%
- **Horizon optimal**: 7-30 jours (au-delÃ , incertitude augmente)

### UtilitÃ© MÃ©tier

âœ… **Planification stratÃ©gique**: Anticiper besoins abattoirs 30 jours
âœ… **Optimisation logistique**: RÃ©server crÃ©neaux transport
âœ… **Gestion stocks**: PrÃ©voir emballages, conditionnements
âœ… **NÃ©gociations commerciales**: Garantir volumes aux clients

---

## 2ï¸âƒ£ Clustering Gaveurs (K-Means)

### ğŸ¤– Est-ce de l'IA/ML ? **OUI âœ…**

**Algorithme**: K-Means (Clustering non supervisÃ©)
**Type**: Machine Learning - Apprentissage non supervisÃ©
**ImplÃ©mentation**: `backend-api/app/ml/euralis/gaveur_clustering.py`

### Technologie

- **BibliothÃ¨que**: `scikit-learn` (Python)
- **Algorithme**: K-Means++ (initialisation intelligente)
- **Normalisation**: StandardScaler (moyenne=0, Ã©cart-type=1)
- **Nombre clusters**: K=5 (paramÃ©trable)

### Comment Ã§a fonctionne

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# 1. PrÃ©parer features
features = [
    'itm_moyen',        # Performance conversion
    'sigma_moyen',      # HomogÃ©nÃ©itÃ© lots
    'mortalite_moyenne',# Taux mortalitÃ©
    'nb_lots',          # ExpÃ©rience
    'regularite'        # Variance ITM
]

X = gaveurs_df[features]

# 2. Normaliser (indispensable pour K-Means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. K-Means avec K=5
kmeans = KMeans(
    n_clusters=5,
    init='k-means++',  # Initialisation intelligente
    n_init=10,         # 10 tentatives
    max_iter=300,
    random_state=42
)

# 4. Assigner clusters
clusters = kmeans.fit_predict(X_scaled)
```

### Algorithme K-Means en DÃ©tail

**Initialisation K-Means++**:
1. Choisir 1er centre alÃ©atoirement
2. Pour centres suivants, choisir points Ã©loignÃ©s des centres existants (probabilitÃ© âˆ distanceÂ²)
3. RÃ©pÃ©ter jusqu'Ã  K centres

**ItÃ©rations**:
```
RÃ©pÃ©ter jusqu'Ã  convergence:
  1. Assigner chaque gaveur au centre le plus proche
     cluster(i) = argmin_k ||xi - Î¼k||Â²

  2. Recalculer centres (barycentres)
     Î¼k = (1/|Ck|) Î£(iâˆˆCk) xi
```

### RÃ©sultats Produits

```json
{
  "cluster": 0,
  "label": "Excellent",
  "gaveur_id": 12,
  "nom": "Sophie Dubois",
  "performance_score": 0.92,
  "itm_moyen": 13.5,
  "mortalite": 1.8,
  "recommendation": "Top performer - Partager bonnes pratiques"
}
```

**Distribution typique**:
- **Cluster 0 (Excellent)**: 6 gaveurs - ITM < 14, MortalitÃ© < 2%
- **Cluster 1 (TrÃ¨s bon)**: 10 gaveurs - ITM 14-15.5, MortalitÃ© 2-3%
- **Cluster 2 (Bon)**: 15 gaveurs - ITM 15.5-17, MortalitÃ© 3-4%
- **Cluster 3 (Ã€ amÃ©liorer)**: 7 gaveurs - ITM 17-20, MortalitÃ© 4-6%
- **Cluster 4 (Critique)**: 2 gaveurs - ITM > 20, MortalitÃ© > 6%

### UtilitÃ© MÃ©tier

âœ… **Identification top performers**: Mettre en avant, partager pratiques
âœ… **Accompagnement ciblÃ©**: Formation gaveurs cluster 3-4
âœ… **Benchmarking**: Comparer gaveur Ã  son cluster
âœ… **Primes diffÃ©renciÃ©es**: Objectifs adaptÃ©s par cluster

---

## 3ï¸âƒ£ DÃ©tection Anomalies (Isolation Forest)

### ğŸ¤– Est-ce de l'IA/ML ? **OUI âœ…**

**Algorithme**: Isolation Forest (ForÃªt d'isolation)
**Type**: Machine Learning - DÃ©tection d'outliers
**ImplÃ©mentation**: `backend-api/app/ml/euralis/anomaly_detection.py`

### Technologie

- **BibliothÃ¨que**: `scikit-learn` (Python)
- **Algorithme**: ForÃªt d'arbres binaires alÃ©atoires
- **Contamination**: 10% (proportion anomalies attendues)
- **N_estimators**: 100 arbres

### Comment Ã§a fonctionne

```python
from sklearn.ensemble import IsolationForest

# 1. Features pour dÃ©tection
features = lots_df[[
    'itm',
    'sigma',
    'pctg_perte_gavage',  # MortalitÃ©
    'duree_gavage_reelle',
    'total_corn_real'
]]

# 2. Isolation Forest
iso_forest = IsolationForest(
    contamination=0.1,     # 10% anomalies
    n_estimators=100,      # 100 arbres
    max_samples=256,
    random_state=42
)

# 3. DÃ©tecter anomalies
predictions = iso_forest.fit_predict(features)
# -1 = anomalie, +1 = normal

# 4. Scores anomalie
anomaly_scores = iso_forest.score_samples(features)
# Plus nÃ©gatif = plus anormal
```

### Principe de l'Isolation Forest

**Intuition**: Les anomalies sont **faciles Ã  isoler** (peu de branches dans l'arbre).

1. **Construction d'un arbre**:
   ```
   Choisir feature alÃ©atoire (ex: ITM)
   Choisir seuil alÃ©atoire entre min et max
   SÃ©parer donnÃ©es: ITM < seuil â†’ gauche, ITM â‰¥ seuil â†’ droite
   RÃ©pÃ©ter rÃ©cursivement jusqu'Ã  isoler chaque point
   ```

2. **Score anomalie**:
   ```
   h(x) = profondeur moyenne pour isoler x dans 100 arbres

   Score anomalie = 2^(-h(x)/c(n))

   OÃ¹ c(n) = 2ln(n-1) + 0.5772 (constante normalisation)
   ```

3. **DÃ©cision**:
   - Score > 0.6 â†’ Anomalie
   - Score â‰ˆ 0.5 â†’ Normal
   - Score < 0.4 â†’ TrÃ¨s normal

### Exemple Concret

**Lot anormal dÃ©tectÃ©**:
```json
{
  "lot_id": 42,
  "code_lot": "LL240815",
  "anomaly_score": -0.52,
  "is_anomaly": true,
  "raisons": [
    "ITM anormal: 25.3 (moyenne: 15.2, Ã©cart: +10.1)",
    "MortalitÃ© Ã©levÃ©e: 8.1% (90e percentile: 4.5%)",
    "Poids foie faible: 420g (10e percentile: 480g)"
  ]
}
```

**Lot normal**:
```json
{
  "lot_id": 15,
  "code_lot": "LL240703",
  "anomaly_score": 0.12,
  "is_anomaly": false
}
```

### UtilitÃ© MÃ©tier

âœ… **Alertes prÃ©coces**: DÃ©tecter lots problÃ©matiques avant abattage
âœ… **Investigation ciblÃ©e**: Contacter gaveur pour comprendre causes
âœ… **AmÃ©lioration continue**: Analyser post-mortem, Ã©viter rÃ©currence
âœ… **PrÃ©vention pertes**: Intervenir avant dÃ©gradation qualitÃ©

---

## 4ï¸âƒ£ Optimisation Abattages (Hungarian Algorithm)

### ğŸ¤– Est-ce de l'IA/ML ? **NON âŒ (Mais intelligent)**

**Algorithme**: Algorithme Hongrois (Kuhn-Munkres)
**Type**: Recherche opÃ©rationnelle - Optimisation combinatoire
**ImplÃ©mentation**: `backend-api/app/ml/euralis/abattage_optimization.py`

### Pourquoi ce n'est PAS du ML

- **Pas d'apprentissage**: Pas de phase d'entraÃ®nement sur donnÃ©es
- **DÃ©terministe**: MÃªme entrÃ©e â†’ Toujours mÃªme sortie
- **Pas de gÃ©nÃ©ralisation**: RÃ©sout problÃ¨me spÃ©cifique, pas de prÃ©diction

**MAIS** c'est un **algorithme intelligent** d'optimisation mathÃ©matique.

### Technologie

- **BibliothÃ¨que**: `scipy.optimize.linear_sum_assignment`
- **ComplexitÃ©**: O(nÂ³) - Polynomial (rÃ©solvable en temps raisonnable)
- **Type problÃ¨me**: Assignment problem (affectation optimale)

### Comment Ã§a fonctionne

```python
from scipy.optimize import linear_sum_assignment

# 1. Construire matrice coÃ»ts (lots Ã— slots abattoirs)
# CoÃ»t = distance + urgence + pÃ©nalitÃ© surcharge

cost_matrix = np.zeros((n_lots, n_slots))

for i, lot in enumerate(lots):
    for j, slot in enumerate(slots):
        # Distance transport
        distance_cost = distances[(lot['site'], slot['abattoir'])]

        # Urgence (jours depuis fin gavage)
        urgence_cost = lot['urgence'] * 100

        # PÃ©nalitÃ© surcharge si capacitÃ© dÃ©passÃ©e
        if lot['nb_canards'] > slot['capacity']:
            overflow_cost = 10000  # TrÃ¨s pÃ©nalisant
        else:
            overflow_cost = 0

        cost_matrix[i, j] = distance_cost + urgence_cost + overflow_cost

# 2. RÃ©soudre avec algorithme hongrois
row_ind, col_ind = linear_sum_assignment(cost_matrix)

# 3. Affectation optimale
planning = {
    lots[i]['id']: (slots[j]['abattoir'], slots[j]['date'])
    for i, j in zip(row_ind, col_ind)
}
```

### Principe Algorithme Hongrois

**ProblÃ¨me**: Affecter N lots Ã  N slots pour minimiser coÃ»t total.

**Ã‰tapes**:

1. **RÃ©duction lignes**: Soustraire min de chaque ligne
   ```
   C'[i,j] = C[i,j] - min(C[i,:])
   ```

2. **RÃ©duction colonnes**: Soustraire min de chaque colonne
   ```
   C''[i,j] = C'[i,j] - min(C'[:,j])
   ```

3. **Couverture zÃ©ros**: Tracer lignes/colonnes pour couvrir tous les 0
   - Si N lignes â†’ Solution trouvÃ©e (assigner aux 0)
   - Sinon â†’ Ajuster matrice et recommencer

4. **Solution optimale**: Affectation qui minimise coÃ»t total

### Exemple Planning GÃ©nÃ©rÃ©

```json
{
  "date_abattage": "2026-01-15",
  "abattoir": "Landes",
  "capacity": 600,
  "lots": [
    {
      "code_lot": "LL240801",
      "nb_canards": 200,
      "gaveur": "Sophie Dubois",
      "distance_km": 15,
      "urgence": 2
    },
    {
      "code_lot": "LL240805",
      "nb_canards": 250,
      "gaveur": "Jean Martin",
      "distance_km": 12,
      "urgence": 1
    },
    {
      "code_lot": "LL240810",
      "nb_canards": 150,
      "gaveur": "Marie Petit",
      "distance_km": 18,
      "urgence": 3
    }
  ],
  "total_canards": 600,
  "efficiency_score": 0.98,
  "total_distance_km": 45,
  "avg_urgence": 2.0
}
```

### Contraintes IntÃ©grÃ©es

- âœ… **CapacitÃ© abattoirs**: Ne pas dÃ©passer limite quotidienne
- âœ… **FraÃ®cheur lots**: PrivilÃ©gier lots arrivÃ©s Ã  maturitÃ©
- âœ… **Distance minimale**: RÃ©duire coÃ»ts transport
- âœ… **Ã‰quilibrage sites**: RÃ©partir entre LL, LS, MT

### UtilitÃ© MÃ©tier

âœ… **+15% efficacitÃ© logistique**: Moins de trajets Ã  vide
âœ… **Planification 7 jours**: Vision claire pour coordination
âœ… **Maximisation remplissage**: Abattoirs utilisÃ©s Ã  95%+
âœ… **RÃ©duction coÃ»ts transport**: Optimisation km parcourus

---

## 5ï¸âƒ£ CorrÃ©lations Globales (Pearson)

### ğŸ¤– Est-ce de l'IA/ML ? **NON âŒ (Mais analytique)**

**Algorithme**: Coefficient de corrÃ©lation de Pearson
**Type**: Statistique descriptive
**ImplÃ©mentation**: `euralis-frontend/app/euralis/analytics/page.tsx`

### Pourquoi ce n'est PAS du ML

- **Pas d'apprentissage**: Calcul statistique simple
- **Pas de prÃ©diction**: Mesure association entre variables
- **Calcul direct**: Formule mathÃ©matique, pas d'optimisation

**MAIS** c'est une **analyse statistique puissante** pour identifier leviers.

### Technologie

- **Frontend**: React + D3.js (visualisation)
- **Calcul**: JavaScript (cÃ´tÃ© client)
- **DonnÃ©es**: 58 lots CSV rÃ©els Euralis 2024

### Comment Ã§a fonctionne

```javascript
// Coefficient de corrÃ©lation de Pearson
function pearsonCorrelation(x, y) {
  const n = x.length;

  // 1. Moyennes
  const meanX = x.reduce((a, b) => a + b, 0) / n;
  const meanY = y.reduce((a, b) => a + b, 0) / n;

  // 2. Covariance et variances
  let num = 0;   // Î£(xi - xÌ„)(yi - È³)
  let denX = 0;  // Î£(xi - xÌ„)Â²
  let denY = 0;  // Î£(yi - È³)Â²

  for (let i = 0; i < n; i++) {
    const dx = x[i] - meanX;
    const dy = y[i] - meanY;
    num += dx * dy;
    denX += dx * dx;
    denY += dy * dy;
  }

  // 3. Coefficient r
  return num / Math.sqrt(denX * denY);
}
```

### Formule MathÃ©matique

```
Coefficient de Pearson r:

        Î£(xi - xÌ„)(yi - È³)
r = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    âˆš[Î£(xi - xÌ„)Â²] âˆš[Î£(yi - È³)Â²]

OÃ¹:
- xÌ„, È³ = moyennes
- r âˆˆ [-1, +1]
- r > 0 â†’ CorrÃ©lation positive
- r < 0 â†’ CorrÃ©lation nÃ©gative
- |r| > 0.3 â†’ CorrÃ©lation significative
```

### Variables AnalysÃ©es

**7 variables sur 58 lots CSV**:

1. **ITM** (Performance) - Conversion maÃ¯s â†’ foie
2. **Sigma** (Performance) - HomogÃ©nÃ©itÃ© lot
3. **Total corn** (Gavage) - Dose totale maÃ¯s consommÃ©e
4. **Nb morts** (Gavage) - MortalitÃ© en gavage
5. **Poids foie rÃ©el** (QualitÃ©) - Poids moyen foies
6. **DurÃ©e gavage** (Gavage) - Nombre jours
7. **Nb canards** (Lot) - Taille du lot

### Exemples CorrÃ©lations DÃ©tectÃ©es

**CorrÃ©lation nÃ©gative forte** (r = -0.72):
```
ITM â†‘ âŸº Poids foie â†“

InterprÃ©tation: Plus l'ITM est Ã©levÃ©, plus le poids de foie est faible
â†’ Mauvaise conversion alimentaire
â†’ Levier: Optimiser courbe gavage pour rÃ©duire ITM
```

**CorrÃ©lation positive modÃ©rÃ©e** (r = +0.45):
```
Sigma â†‘ âŸº Nb morts â†‘

InterprÃ©tation: Lots hÃ©tÃ©rogÃ¨nes ont plus de mortalitÃ©
â†’ Importance de l'homogÃ©nÃ©itÃ© au dÃ©marrage
â†’ Levier: Tri canards plus strict avant gavage
```

**CorrÃ©lation positive forte** (r = +0.85):
```
Total corn â†‘ âŸº DurÃ©e gavage â†‘

InterprÃ©tation: Plus on gave longtemps, plus on consomme
â†’ Relation logique, pas d'action
```

### Visualisation Network Graph

**Frontend D3.js**:
- **NÅ“uds**: Variables (colorÃ©es par catÃ©gorie)
- **Liens**: CorrÃ©lations significatives (|r| > 0.3)
  - Vert: CorrÃ©lation positive
  - Rouge: CorrÃ©lation nÃ©gative
  - Ã‰paisseur: Proportionnelle Ã  |r|

### UtilitÃ© MÃ©tier

âœ… **Identification leviers**: Quelles variables actionner pour amÃ©liorer
âœ… **Robustesse statistique**: 58 lots â†’ CorrÃ©lations fiables
âœ… **Benchmarking inter-gaveurs**: Comparer pratiques
âœ… **Formations ciblÃ©es**: Conseils data-driven

---

## ğŸ”¬ RÃ©capitulatif: IA vs. Non-IA

| Module | IA/ML ? | Type | BibliothÃ¨que | Apprentissage |
|--------|---------|------|--------------|---------------|
| **PrÃ©visions** | âœ… OUI | ML - SÃ©ries temporelles | Prophet | OUI - EntraÃ®nement sur historique |
| **Clustering** | âœ… OUI | ML - Non supervisÃ© | Scikit-learn | OUI - K-Means trouve centres optimaux |
| **Anomalies** | âœ… OUI | ML - Outlier detection | Scikit-learn | OUI - ForÃªt d'arbres alÃ©atoires |
| **Optimisation** | âŒ NON | Recherche opÃ©rationnelle | SciPy | NON - Algorithme dÃ©terministe |
| **CorrÃ©lations** | âŒ NON | Statistique descriptive | JavaScript | NON - Calcul direct formule |

### DÃ©finition Machine Learning

**Un systÃ¨me fait du ML s'il**:
1. **Apprend** Ã  partir de donnÃ©es (phase d'entraÃ®nement)
2. **GÃ©nÃ©ralise** sur nouvelles donnÃ©es (prÃ©dictions)
3. **AmÃ©liore** ses performances avec plus de donnÃ©es

**Prophet, K-Means, Isolation Forest** â†’ âœ… Respectent ces critÃ¨res
**Hungarian, Pearson** â†’ âŒ Calculs directs, pas d'apprentissage

---

## ğŸ“ˆ Impact Business

### ROI EstimÃ©

**Avec analytics intelligents**:
- ğŸ¯ **RÃ©duction ITM -10%** â†’ Ã‰conomie ~100kg maÃ¯s/lot
- ğŸ¯ **RÃ©duction mortalitÃ© -20%** â†’ +50-100 foies vendables/lot
- ğŸ¯ **Optimisation logistique +15%** â†’ -30% coÃ»ts transport

**Calcul ROI (base 500 lots/an)**:
```
Ã‰conomie maÃ¯s:     500 lots Ã— 100kg Ã— 0.30â‚¬/kg = 15 000â‚¬/an
Foies sauvÃ©s:      500 lots Ã— 75 foies Ã— 8â‚¬      = 300 000â‚¬/an
Transport:         CoÃ»ts actuels 80 000â‚¬ Ã— 30%   =  24 000â‚¬/an
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total ROI estimÃ©:                                 339 000â‚¬/an
```

### BÃ©nÃ©fices Qualitatifs

âœ… **DÃ©cisions data-driven**: Fini l'intuition, place aux faits
âœ… **RÃ©activitÃ©**: Alertes temps rÃ©el sur anomalies
âœ… **Accompagnement personnalisÃ©**: Conseils adaptÃ©s par cluster
âœ… **TraÃ§abilitÃ©**: Toutes dÃ©cisions ML justifiÃ©es par data

---

## ğŸ› ï¸ Maintenance & Ã‰volutions

### RÃ©entraÃ®nement ModÃ¨les

**FrÃ©quence recommandÃ©e**:
- **Prophet**: Tous les 3 mois (nouvelles tendances)
- **K-Means**: Tous les 6 mois (nouveaux gaveurs, Ã©volution pratiques)
- **Isolation Forest**: Mensuel (adaptation seuils anomalies)

**ProcÃ©dure**:
```bash
# 1. Extraire donnÃ©es rÃ©centes
python scripts/extract_training_data.py --since 2025-01-01

# 2. RÃ©entraÃ®ner modÃ¨les
python backend-api/app/ml/euralis/retrain_all.py

# 3. Ã‰valuer performances
python backend-api/app/ml/euralis/evaluate_models.py

# 4. DÃ©ployer si MAPE < seuil
python scripts/deploy_models.py
```

### AmÃ©liorations Futures

**Phase 4 (Q2 2026)**:
- ğŸ”® **Deep Learning**: LSTM pour prÃ©visions long terme (90+ jours)
- ğŸ”® **Reinforcement Learning**: Optimiser courbes gavage automatiquement
- ğŸ”® **AutoML**: HyperparamÃ¨tres optimisÃ©s automatiquement
- ğŸ”® **Explainability**: SHAP values pour expliquer prÃ©dictions

---

## ğŸ“š RÃ©fÃ©rences

### BibliothÃ¨ques UtilisÃ©es

- **Prophet**: Taylor, S.J., Letham, B. (2018). Forecasting at Scale. The American Statistician.
- **Scikit-learn**: Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python. JMLR.
- **SciPy**: Virtanen et al. (2020). SciPy 1.0: Fundamental Algorithms for Scientific Computing.
- **D3.js**: Bostock, M. (2011). D3: Data-Driven Documents. IEEE Visualization.

### Documentation Technique

- `backend-api/app/ml/euralis/` - Code source modules ML
- `CLAUDE.md` - Architecture systÃ¨me complÃ¨te
- `INTEGRATION_CSV_SQAL_COMPLETE.md` - IntÃ©gration donnÃ©es
- `GUIDE_DEMO_CLIENT.md` - DÃ©monstration client

---

**DerniÃ¨re mise Ã  jour**: 2026-01-13
**Auteur**: SystÃ¨me Gaveurs V3.0
**Contact**: Support Technique Euralis
